{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Thermotar","text":""},{"location":"#quickstart","title":"Quickstart","text":""},{"location":"#installation","title":"Installation","text":"<p>Run from the root directory of the project: <pre><code>pip install .\n</code></pre> Alternatively build without explicitly cloning first: <pre><code>pip install git+https://github.com/chappertron/thermotar\n</code></pre></p> <p>Currently, there is no pip or conda distribution. </p>"},{"location":"#examples","title":"Examples","text":"<pre><code>  import thermotar as th\n  # Read LAMMPS log file\n  # By default reads the last runs output\n  thermo = th.create_thermos('log.lammps')\n\n  import matplotlib.pyplot as plt\n  plt.plot(thermo.Step,thermo.Temp)\n  plt.show()\n\n  # Read data from chunk ave or RDF\n  # Reads only the last table\n  chunk = th.create_chunk('chunk.prof')\n  plt.plot(chunk.Coord1,chunk.temp)\n  plt.show()\n\n  # Read all the chunks from a chunk/ave file\n  chunks = th.create_multi_chunks('chunk.prof')\n  # Average the different frames of multichunk into a single chunk\n  chunk = chunks.flatten_chunk()\n</code></pre>"},{"location":"#full-documentation","title":"Full Documentation","text":"<p>The reference section contains documentation for the main package.</p> <p>Useful places to look are: - <code>thermo</code> - <code>chunk</code> - <code>interface</code></p>"},{"location":"reference/chunk/","title":"Chunk","text":""},{"location":"reference/chunk/#chunk","title":"Chunk","text":"<p>Class for Reading output from LAMMPS chunk/ave command.</p> Source code in <code>thermotar/chunk.py</code> <pre><code>class Chunk:\n    \"\"\"Class for Reading output from LAMMPS chunk/ave command.\"\"\"\n\n    def __init__(\n        self,\n        thermo_df: pd.DataFrame,\n        CLEANUP=True,\n        coord_cols=[\"Coord1\", \"Coord2\", \"Coord3\", \"coord\", \"Box\"],\n        centred=False,\n        centered=None,\n        **kwargs,\n    ):\n        \"\"\"\n        Construct a `Chunk`  from a pandas Dataframe.\n\n        Parameters\n        ----------\n        thermo_df :\n            Data frame to read values from.\n        CLEANUP :\n            If true, the headers of the DataFrame are tided up to become valid python\n            identifiers and strips the prefixes from compute/fix and variable columns.\n        centred :\n            Whether the coordinates of the system are already centred. This option\n            will be deprecated; the centring calculation is cheap.\n\n        \"\"\"\n        self.data: pd.DataFrame = thermo_df\n\n        # clean up dataframe\n\n        # apply strip_pref function to remove 'c_/f_/v_' prefixes to all columns\n        if CLEANUP:\n            # TODO: the prefix stripping should not be done if there ends up being\n            # ambiguity between the fields; for example if c_foo and f_foo are both\n            # defined\n            self.data.rename(columns=lmp_utils.strip_pref, inplace=True)\n            self.data.rename(columns=lmp_utils.drop_python_bad, inplace=True)\n\n        # set the columns as attributes\n        for col in self.data.columns:\n            # has to be set to a method of the class\n            setattr(\n                self.__class__, col, raise_col(self, col)\n            )  # set attribute to this property object??\n        # column names for the coordinates, up to 3\n        # only those in the df are included, by finding intersect of sets.\n        self.coord_cols = list(set(self.data.columns.to_list()) &amp; set(coord_cols))\n        if centered is not None:\n            centred = centered\n\n        self.centred = centred  # Initialise assuming asymmetrical - to do implement a method to check this!!!!\n\n    # Property Definitions\n\n    @property\n    def centered(self) -&gt; bool:\n        \"\"\"Return whether the coordinates have already been centred.\"\"\"\n        return self.centred\n\n    @classmethod\n    def create_chunk(\n        cls, fname: Union[str, PathLike], style: str = \"lmp\", last: bool = True\n    ):\n        \"\"\"\n        Load LAMMPS or numpy savetxt as a df and then create a Chunk instance.\n\n        Parameters\n        ----------\n        fname:\n            File to load.\n        style:\n            What is the format of the file.\n            Supported values are \"lmp\" and \"np\", for lammps chunkfiles and numpy\n            savetxt output, respectively.\n        last:\n            Only true is supported. Whether to read the last frame of the chunk file\n            or all of them. Use `MultiChunk` if all frames are needed.\n        \"\"\"\n        # TODO: Implement .xvg styles\n        # TODO: Implement auto choosing the style, either with file extensions\n        # or some form of try and fail\n\n        # if style == 'auto':\n        #     try:\n        #         with open(fname,'r') as stream:\n        #             for i,line in enumerate(stream):\n        #                 if i==3: int(line.split()[1]) # try to cast into an integer, if this fails, likely not lmp style\n        #         style = 'lmp'\n        #     except ValueError:\n        #         # set style = 'np'\n        #         style = 'np'\n        if style == \"lmp\":\n            if last:\n                df = parse_chunks.lmp_last_chunk(fname, nchunks=\"auto\", header_rows=3)\n            else:\n                raise NotImplementedError(\n                    \"Only the last chunk is supported at the moment.\"\n                )\n        elif style == \"np\":\n            # read the data frame from a space separated file outputted by np.savetxt.\n            df = parse_chunks.parse_numpy_file(fname, header_row=0, comment=\"#\")\n        else:\n            raise NotImplementedError(\n                \"Only LAMMPS chunk files and numpy save_txt files are supported so far.\"\n            )\n\n        return cls(df)\n\n    def raise_columns(self):\n        \"\"\"Raise columns from the df to be attributes.\"\"\"\n        # I have no clue how pandas does this automatically...\n        # Maybe I need to make it so my objects can be indexed\n        # and in doing that for assignment, the attributes can be raised up\n        # TODO : Something with above??\n        df_utils.raise_columns(self)\n\n    def prop_grad(self, prop: str, coord: str, **kwargs):\n        \"\"\"Calculate the gradient of `prop` with respect to `coord`.\n\n        Parameters\n        ----------\n        prop:\n            Which property the gradient of is calculated\n        coord:\n            Which property is used for the coordinate.\n        kwargs:\n            Keyword arguments to pass to `np.gradient`\n        \"\"\"\n        df = self.data\n\n        df[prop + \"_grad\"] = np.gradient(df[prop], df[coord], **kwargs)\n\n        # updates the columns\n        df_utils.raise_columns(self)\n\n    def nearest(self, property: str, value: float, coord=0):\n        \"\"\"Return the index for which property is closest to `value`.\n\n        To do, return the actual row of properties??????\n\n        This way if nan - make a nan list\n\n        \"\"\"\n        # TODO: Find out where this method is used and understand what on earth it does\n        coord = self.data[\n            self.coord_cols[coord]\n        ]  # select coordinate column for which to find nearsest in each side\n\n        prop = self.data[property]\n        # if each half, find the index in each half of the simulation box\n\n        left = prop.loc[coord &lt; 0]\n        right = prop.loc[coord &gt; 0]\n\n        right_nearest_index = right.sub(value).abs().idxmin()\n\n        if len(left.index) != 0:\n            left_nearest_index = left.sub(value).abs().idxmin()\n            indexes = [left_nearest_index, right_nearest_index]\n        else:\n            indexes = [right_nearest_index]\n\n        try:\n            return self.data.loc[indexes].copy()\n        except KeyError:  # just create an empty row\n            row_copy = self.data.loc[\n                (0,), :\n            ].copy()  # janky indexing makes it a df rather than a series\n            row_copy.loc[:] = np.nan\n            return row_copy\n\n    def centre(\n        self, coord: Union[int, str, List[str]] = \"all\", moment: Optional[int] = None\n    ):\n        \"\"\"Shift the origin of the simulation box to zero.\n\n        Parameters\n        ----------\n        coord:\n            Index of coordinate column to centre , indexes self.coord_cols.\n            Default is 'all'.\n\n        moment:\n            If Not None, centres the system to this column name,\n            weighted by this column name raised to the power of moment.\n\n        \"\"\"\n        if coord == \"all\":\n            # calculate the union of the list of coord_cols and the df columns\n            coords = self.coord_cols\n\n        elif isinstance(coord, int):\n            coords = [self.coord_cols[coord]]\n        else:\n            # if neither a number or 'all', assumes a column name\n            coords = [coord]\n        # iterate over selected coordinates and perform the centring operation\n        for coord_col in coords:\n            if moment:\n                self.data[coord_col] -= self.moment(\n                    coord_col, moment\n                )  # set origin to be first moment, weighted by moment parameter\n            else:\n                # print(coord_col)\n                self.data[coord_col] = self.centre_series(self.data[coord_col])\n\n        self.centred = True\n        # Return the object at the end for method chaining\n        return self\n\n    def center(self, coord=\"all\"):\n        \"\"\"An alias of centre for yanks.\"\"\"\n        return self.centre(coord=coord)\n\n    @staticmethod\n    def centre_series(series: pd.Series):\n        \"\"\"Subtract the average of a series from the series.\"\"\"\n        centre = (series.max() + series.min()) / 2\n        centred = series - centre\n        return centred\n\n    def parity(self, prop, coord=0):\n        \"\"\"\n        Multiplies a property by the sign of the coordinate.\n\n        Should only be applied to properties that are pseudo scalars,  i.e. change\n        sign under coordinate inversion, so that upon folding and averaging\n        properties are correct.\n        \"\"\"\n        # centre first\n        if isinstance(coord, int):\n            coord = self.coord_cols[coord]\n        if not self.centred:\n            self.centre()\n\n        self.data[prop + \"_original\"] = self.data[prop]\n        self.data[prop] *= np.sign(\n            self.data[coord]\n        )  # multiply by the sign of the coordinate column\n        self.raise_columns()\n\n    def moment(\n        self,\n        coord,\n        weighting,\n        order=1,\n    ):\n        \"\"\"Calculate the specified moment of the coordinate, weighted by a named property\"\"\"\n        coords = self.data[self.choose_coordinate(coord)].T\n\n        integrand = self.data[weighting] * coords**order\n        normaliser = np.trapz(self.data[weighting], coords)\n\n        return np.trapz(integrand, coords) / normaliser\n\n    def choose_coordinate(self, coord: Union[int, str]):\n        \"\"\"Find the provided coordinate column(s).\n\n        If an integer, indexes the self.coord_cols field,\n        if string 'all', returns self.coord_cols\n        if any other, returns\n        \"\"\"\n        if coord == \"all\":\n            # calculate the union of the list of coord_cols and the df columns\n            coords = self.coord_cols\n\n        elif isinstance(coord, int):\n            coords = [self.coord_cols[coord]]\n        else:\n            # if neither a number or 'all', assumes a column name\n            coords = [coord]\n\n        return coords\n\n    def fold(self, crease=0.0, coord=None, coord_i: int = 0, sort=False, inplace=True):\n        \"\"\"\n        Fold the profile about coord = crease.\n\n        WARNING: if properties have been calculated prior to folding, they may no\n        longer be correct.\n\n        For example, electric fields calculated by integrating charge profiles,\n        will have a different sign in each part of the box.\n\n        To deal with this they should be inverted based on the sign of the coordiante.\n\n        Parameters\n        ----------\n        crease: -\n                Position along folded coordinate to fold about\n\n        coord_i : int -\n                The index of the self.coord_cols list. Default is 0, the first coord\n        \"\"\"\n        if coord is None:\n            coord_name = self.coord_cols[coord_i]\n        else:\n            coord_name = coord\n\n        if (crease == 0) and self.centred:\n            # don't bother finding fold line, just go straight to folding!!!\n            # in this case fold by making all negative and\n            self.data[coord_name] = np.absolute(self.data[coord_name])\n        elif crease == 0:\n            # if the crease is located at coord = 0, but not already centred then - centres\n            self.centre(coord=coord_name)\n            self.data[coord_name] = np.absolute(self.data[coord_name])\n        else:\n            # folding about some other value\n            self.data[coord_name] -= (\n                crease  # set origin to the value to be creased about\n            )\n            self.data[coord_name] = np.absolute(\n                self.data[coord_name]\n            )  # get absolute value\n            self.data[coord_name] += (\n                crease  # add the crease value back so still starts at this value!\n            )\n\n        # TODO Implement fully, even when not centred!\n\n        # TODO auto sort ?\n\n        if sort:\n            # sorts the data by this column name\n            self.data.sort_values(\n                by=coord_name, inplace=True, ignore_index=True\n            )  # labels 0,1,2.... Needed for future integration/differentaion operations in numpy\n\n        pass\n\n    def rebin(\n        self,\n        coord,\n        bw=0.25,\n        bins=None,\n        nbins=None,\n        mode=\"average\",\n        inplace=False,\n        new_coord_loc=\"mid\",\n        weights=None,\n    ):\n        \"\"\"\n        Rebin the data based on coordinates for a given new bin width.\n\n        Default is to perform averages over these bins.\n        Could also add weightings for these averages\n\n        Parameters\n        ----------\n        coord:\n            Column name of the coordinate to create the bins from\n\n        nbins:\n            None, int, or array of ints. Number of bins for each binning dimension.\n            Currently only supported for 2D bins.\n\n        new_coord_loc:\n            \"mid\" (default), \"left\" or \"right\" position of new coordinate,\n            when set manually rather than from average. Currently only for 2D bins\n\n        inplace : bool\n            if True, overwrites the .data method, else just returns the data frame.\n\n        weights:\n            Column label for performing a weighted average,\n            only used if mode is \"average\" or \"mean\"\n\n        \"\"\"\n        # TODO: - implement n_bins argument for 1d bins\n        df = self.data\n\n        # Use multiple bins\n        if np.iterable(coord) and not isinstance(coord, str):\n            number_coords = len(coord)\n            if number_coords == 2:\n                coord1, coord2 = coord[0], coord[1]\n                df_binned = df_utils.rebin_2D(\n                    df,\n                    coord1,\n                    coord2,\n                    bw=bw,\n                    bins=bins,\n                    mode=mode,\n                    nbins=nbins,\n                    new_coord_loc=new_coord_loc,\n                    weight_col=weights,\n                )\n            elif np.number &gt;= 3:\n                raise NotImplementedError(\n                    \"Binning in more than two dimensions not yet supported\"\n                )\n        else:\n            df_binned = df_utils.rebin(\n                df, coord, bw=bw, mode=mode, weight_col=weights, bins=bins\n            )\n        # coord_max = df[coord].max()\n        # coord_min = df[coord].min() # finding min and max so it works with gfolded data\n\n        # n_bins = (coord_max-coord_min) // bw #double divide is floor division\n        # n_bins = int(n_bins)\n\n        # bins = pd.cut(df[coord],n_bins)\n\n        # df_grouped = df.groupby(bins)\n\n        # if mode == 'average' or mode == 'mean':\n        #     df_binned = df_grouped.mean()\n        # else:\n        #     df_binned = df_grouped.sum()\n\n        # TODO:  Why is df_binned possibly unbound?\n        if inplace:\n            self.data = df_binned\n        else:\n            return df_binned\n\n    def fold_and_ave(\n        self,\n        crease=0.0,\n        coord=None,\n        coord_i=0,\n        sort=True,\n        bw: Union[str, float] = \"auto\",\n    ):\n        \"\"\"Fold the profile and average the two halves.\n\n            WARNING: if properties have been calculated prior to folding,\n            they may no longer be correct, epsecially properties that invert under coordinate inversion (pseudoscalars)\n\n            For example, electric fields calculated by integrating charge profiles, will have a different sign in each part of the box.\n\n        Parameters\n        ----------\n        crease:\n                Position along folded coordinate to fold about\n\n        coord_i : int\n                The index of the self.coord_cols list. Default is 0, the first coord\n                if all, will fold all coordinates, but will only crease\n\n        bw:\n            Averaging works by rebinning\n            If auto, tries to work out the original bin spacing and then groups by this\n            If not auto, specify the bin width in distance.\n\n        \"\"\"\n        if coord is None:\n            if coord_i == \"all\":\n                coord_names = self.coord_cols\n            else:\n                coord_names = [self.coord_cols[coord_i]]\n        else:\n            coord_names = [coord]\n\n        if crease == 0.0 and not self.centred:\n            # if the crease is located at coord = 0, but not already centred then - centres\n            self.centre(coord=coord_names)\n\n        df = self.data.copy()\n\n        coord1 = df[coord_names[0]]\n\n        if bw == \"auto\":\n            bw = np.abs(\n                coord1.iloc[-1] - coord1.iloc[-2]\n            )  # if auto work out from the difference of the last 2 points of the coord\n\n        # # only index by the first coord,but flip all?\n        # select_a = (df[coord_names[0]] &gt;=  0)\n        # select_b = ~select_a\n\n        # df_a = df.loc[select_a].sort_values(by = coord_names[0] ,inplace = True,ignore_index = True)\n        # df_b = df.loc[select_b]\n        # df_b[coord_names]=df_b[coord_names].abs()\n        # df_b.sort_values(by = coord_names[0] ,inplace = True,ignore_index = True)\n\n        # df_ave = pd.concat({'a':df_a,'b':df_b}).mean(level=1)\n\n        # fold the df\n        df[coord_names] = (\n            np.abs(df[coord_names] - crease) + crease\n        )  # assumes already centred for now\n        df_ave = df_utils.rebin(df, coord_names[0], bw).sort_values(\n            coord_names[0]\n        )  # performing the rebinning # then sort by the coord\n\n        return df_ave\n\n    def __getitem__(self, key: str):\n        return self.data[key]\n</code></pre>"},{"location":"reference/chunk/#thermotar.chunk.Chunk.centered","title":"<code>centered: bool</code>  <code>property</code>","text":"<p>Return whether the coordinates have already been centred.</p>"},{"location":"reference/chunk/#thermotar.chunk.Chunk.__init__","title":"<code>__init__(thermo_df, CLEANUP=True, coord_cols=['Coord1', 'Coord2', 'Coord3', 'coord', 'Box'], centred=False, centered=None, **kwargs)</code>","text":"<p>Construct a <code>Chunk</code>  from a pandas Dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>thermo_df</code> <code>DataFrame</code> <p>Data frame to read values from.</p> required <code>CLEANUP</code> <p>If true, the headers of the DataFrame are tided up to become valid python identifiers and strips the prefixes from compute/fix and variable columns.</p> <code>True</code> <code>centred</code> <p>Whether the coordinates of the system are already centred. This option will be deprecated; the centring calculation is cheap.</p> <code>False</code> Source code in <code>thermotar/chunk.py</code> <pre><code>def __init__(\n    self,\n    thermo_df: pd.DataFrame,\n    CLEANUP=True,\n    coord_cols=[\"Coord1\", \"Coord2\", \"Coord3\", \"coord\", \"Box\"],\n    centred=False,\n    centered=None,\n    **kwargs,\n):\n    \"\"\"\n    Construct a `Chunk`  from a pandas Dataframe.\n\n    Parameters\n    ----------\n    thermo_df :\n        Data frame to read values from.\n    CLEANUP :\n        If true, the headers of the DataFrame are tided up to become valid python\n        identifiers and strips the prefixes from compute/fix and variable columns.\n    centred :\n        Whether the coordinates of the system are already centred. This option\n        will be deprecated; the centring calculation is cheap.\n\n    \"\"\"\n    self.data: pd.DataFrame = thermo_df\n\n    # clean up dataframe\n\n    # apply strip_pref function to remove 'c_/f_/v_' prefixes to all columns\n    if CLEANUP:\n        # TODO: the prefix stripping should not be done if there ends up being\n        # ambiguity between the fields; for example if c_foo and f_foo are both\n        # defined\n        self.data.rename(columns=lmp_utils.strip_pref, inplace=True)\n        self.data.rename(columns=lmp_utils.drop_python_bad, inplace=True)\n\n    # set the columns as attributes\n    for col in self.data.columns:\n        # has to be set to a method of the class\n        setattr(\n            self.__class__, col, raise_col(self, col)\n        )  # set attribute to this property object??\n    # column names for the coordinates, up to 3\n    # only those in the df are included, by finding intersect of sets.\n    self.coord_cols = list(set(self.data.columns.to_list()) &amp; set(coord_cols))\n    if centered is not None:\n        centred = centered\n\n    self.centred = centred  # Initialise assuming asymmetrical - to do implement a method to check this!!!!\n</code></pre>"},{"location":"reference/chunk/#thermotar.chunk.Chunk.center","title":"<code>center(coord='all')</code>","text":"<p>An alias of centre for yanks.</p> Source code in <code>thermotar/chunk.py</code> <pre><code>def center(self, coord=\"all\"):\n    \"\"\"An alias of centre for yanks.\"\"\"\n    return self.centre(coord=coord)\n</code></pre>"},{"location":"reference/chunk/#thermotar.chunk.Chunk.centre","title":"<code>centre(coord='all', moment=None)</code>","text":"<p>Shift the origin of the simulation box to zero.</p> <p>Parameters:</p> Name Type Description Default <code>coord</code> <code>Union[int, str, List[str]]</code> <p>Index of coordinate column to centre , indexes self.coord_cols. Default is 'all'.</p> <code>'all'</code> <code>moment</code> <code>Optional[int]</code> <p>If Not None, centres the system to this column name, weighted by this column name raised to the power of moment.</p> <code>None</code> Source code in <code>thermotar/chunk.py</code> <pre><code>def centre(\n    self, coord: Union[int, str, List[str]] = \"all\", moment: Optional[int] = None\n):\n    \"\"\"Shift the origin of the simulation box to zero.\n\n    Parameters\n    ----------\n    coord:\n        Index of coordinate column to centre , indexes self.coord_cols.\n        Default is 'all'.\n\n    moment:\n        If Not None, centres the system to this column name,\n        weighted by this column name raised to the power of moment.\n\n    \"\"\"\n    if coord == \"all\":\n        # calculate the union of the list of coord_cols and the df columns\n        coords = self.coord_cols\n\n    elif isinstance(coord, int):\n        coords = [self.coord_cols[coord]]\n    else:\n        # if neither a number or 'all', assumes a column name\n        coords = [coord]\n    # iterate over selected coordinates and perform the centring operation\n    for coord_col in coords:\n        if moment:\n            self.data[coord_col] -= self.moment(\n                coord_col, moment\n            )  # set origin to be first moment, weighted by moment parameter\n        else:\n            # print(coord_col)\n            self.data[coord_col] = self.centre_series(self.data[coord_col])\n\n    self.centred = True\n    # Return the object at the end for method chaining\n    return self\n</code></pre>"},{"location":"reference/chunk/#thermotar.chunk.Chunk.centre_series","title":"<code>centre_series(series)</code>  <code>staticmethod</code>","text":"<p>Subtract the average of a series from the series.</p> Source code in <code>thermotar/chunk.py</code> <pre><code>@staticmethod\ndef centre_series(series: pd.Series):\n    \"\"\"Subtract the average of a series from the series.\"\"\"\n    centre = (series.max() + series.min()) / 2\n    centred = series - centre\n    return centred\n</code></pre>"},{"location":"reference/chunk/#thermotar.chunk.Chunk.choose_coordinate","title":"<code>choose_coordinate(coord)</code>","text":"<p>Find the provided coordinate column(s).</p> <p>If an integer, indexes the self.coord_cols field, if string 'all', returns self.coord_cols if any other, returns</p> Source code in <code>thermotar/chunk.py</code> <pre><code>def choose_coordinate(self, coord: Union[int, str]):\n    \"\"\"Find the provided coordinate column(s).\n\n    If an integer, indexes the self.coord_cols field,\n    if string 'all', returns self.coord_cols\n    if any other, returns\n    \"\"\"\n    if coord == \"all\":\n        # calculate the union of the list of coord_cols and the df columns\n        coords = self.coord_cols\n\n    elif isinstance(coord, int):\n        coords = [self.coord_cols[coord]]\n    else:\n        # if neither a number or 'all', assumes a column name\n        coords = [coord]\n\n    return coords\n</code></pre>"},{"location":"reference/chunk/#thermotar.chunk.Chunk.create_chunk","title":"<code>create_chunk(fname, style='lmp', last=True)</code>  <code>classmethod</code>","text":"<p>Load LAMMPS or numpy savetxt as a df and then create a Chunk instance.</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>Union[str, PathLike]</code> <p>File to load.</p> required <code>style</code> <code>str</code> <p>What is the format of the file. Supported values are \"lmp\" and \"np\", for lammps chunkfiles and numpy savetxt output, respectively.</p> <code>'lmp'</code> <code>last</code> <code>bool</code> <p>Only true is supported. Whether to read the last frame of the chunk file or all of them. Use <code>MultiChunk</code> if all frames are needed.</p> <code>True</code> Source code in <code>thermotar/chunk.py</code> <pre><code>@classmethod\ndef create_chunk(\n    cls, fname: Union[str, PathLike], style: str = \"lmp\", last: bool = True\n):\n    \"\"\"\n    Load LAMMPS or numpy savetxt as a df and then create a Chunk instance.\n\n    Parameters\n    ----------\n    fname:\n        File to load.\n    style:\n        What is the format of the file.\n        Supported values are \"lmp\" and \"np\", for lammps chunkfiles and numpy\n        savetxt output, respectively.\n    last:\n        Only true is supported. Whether to read the last frame of the chunk file\n        or all of them. Use `MultiChunk` if all frames are needed.\n    \"\"\"\n    # TODO: Implement .xvg styles\n    # TODO: Implement auto choosing the style, either with file extensions\n    # or some form of try and fail\n\n    # if style == 'auto':\n    #     try:\n    #         with open(fname,'r') as stream:\n    #             for i,line in enumerate(stream):\n    #                 if i==3: int(line.split()[1]) # try to cast into an integer, if this fails, likely not lmp style\n    #         style = 'lmp'\n    #     except ValueError:\n    #         # set style = 'np'\n    #         style = 'np'\n    if style == \"lmp\":\n        if last:\n            df = parse_chunks.lmp_last_chunk(fname, nchunks=\"auto\", header_rows=3)\n        else:\n            raise NotImplementedError(\n                \"Only the last chunk is supported at the moment.\"\n            )\n    elif style == \"np\":\n        # read the data frame from a space separated file outputted by np.savetxt.\n        df = parse_chunks.parse_numpy_file(fname, header_row=0, comment=\"#\")\n    else:\n        raise NotImplementedError(\n            \"Only LAMMPS chunk files and numpy save_txt files are supported so far.\"\n        )\n\n    return cls(df)\n</code></pre>"},{"location":"reference/chunk/#thermotar.chunk.Chunk.fold","title":"<code>fold(crease=0.0, coord=None, coord_i=0, sort=False, inplace=True)</code>","text":"<p>Fold the profile about coord = crease.</p> <p>WARNING: if properties have been calculated prior to folding, they may no longer be correct.</p> <p>For example, electric fields calculated by integrating charge profiles, will have a different sign in each part of the box.</p> <p>To deal with this they should be inverted based on the sign of the coordiante.</p> <p>Parameters:</p> Name Type Description Default <code>crease</code> <pre><code>Position along folded coordinate to fold about\n</code></pre> <code>0.0</code> <code>coord_i</code> <code>int -</code> <pre><code>The index of the self.coord_cols list. Default is 0, the first coord\n</code></pre> <code>0</code> Source code in <code>thermotar/chunk.py</code> <pre><code>def fold(self, crease=0.0, coord=None, coord_i: int = 0, sort=False, inplace=True):\n    \"\"\"\n    Fold the profile about coord = crease.\n\n    WARNING: if properties have been calculated prior to folding, they may no\n    longer be correct.\n\n    For example, electric fields calculated by integrating charge profiles,\n    will have a different sign in each part of the box.\n\n    To deal with this they should be inverted based on the sign of the coordiante.\n\n    Parameters\n    ----------\n    crease: -\n            Position along folded coordinate to fold about\n\n    coord_i : int -\n            The index of the self.coord_cols list. Default is 0, the first coord\n    \"\"\"\n    if coord is None:\n        coord_name = self.coord_cols[coord_i]\n    else:\n        coord_name = coord\n\n    if (crease == 0) and self.centred:\n        # don't bother finding fold line, just go straight to folding!!!\n        # in this case fold by making all negative and\n        self.data[coord_name] = np.absolute(self.data[coord_name])\n    elif crease == 0:\n        # if the crease is located at coord = 0, but not already centred then - centres\n        self.centre(coord=coord_name)\n        self.data[coord_name] = np.absolute(self.data[coord_name])\n    else:\n        # folding about some other value\n        self.data[coord_name] -= (\n            crease  # set origin to the value to be creased about\n        )\n        self.data[coord_name] = np.absolute(\n            self.data[coord_name]\n        )  # get absolute value\n        self.data[coord_name] += (\n            crease  # add the crease value back so still starts at this value!\n        )\n\n    # TODO Implement fully, even when not centred!\n\n    # TODO auto sort ?\n\n    if sort:\n        # sorts the data by this column name\n        self.data.sort_values(\n            by=coord_name, inplace=True, ignore_index=True\n        )  # labels 0,1,2.... Needed for future integration/differentaion operations in numpy\n\n    pass\n</code></pre>"},{"location":"reference/chunk/#thermotar.chunk.Chunk.fold_and_ave","title":"<code>fold_and_ave(crease=0.0, coord=None, coord_i=0, sort=True, bw='auto')</code>","text":"<p>Fold the profile and average the two halves.</p> <pre><code>WARNING: if properties have been calculated prior to folding,\nthey may no longer be correct, epsecially properties that invert under coordinate inversion (pseudoscalars)\n\nFor example, electric fields calculated by integrating charge profiles, will have a different sign in each part of the box.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>crease</code> <pre><code>Position along folded coordinate to fold about\n</code></pre> <code>0.0</code> <code>coord_i</code> <code>int</code> <pre><code>The index of the self.coord_cols list. Default is 0, the first coord\nif all, will fold all coordinates, but will only crease\n</code></pre> <code>0</code> <code>bw</code> <code>Union[str, float]</code> <p>Averaging works by rebinning If auto, tries to work out the original bin spacing and then groups by this If not auto, specify the bin width in distance.</p> <code>'auto'</code> Source code in <code>thermotar/chunk.py</code> <pre><code>def fold_and_ave(\n    self,\n    crease=0.0,\n    coord=None,\n    coord_i=0,\n    sort=True,\n    bw: Union[str, float] = \"auto\",\n):\n    \"\"\"Fold the profile and average the two halves.\n\n        WARNING: if properties have been calculated prior to folding,\n        they may no longer be correct, epsecially properties that invert under coordinate inversion (pseudoscalars)\n\n        For example, electric fields calculated by integrating charge profiles, will have a different sign in each part of the box.\n\n    Parameters\n    ----------\n    crease:\n            Position along folded coordinate to fold about\n\n    coord_i : int\n            The index of the self.coord_cols list. Default is 0, the first coord\n            if all, will fold all coordinates, but will only crease\n\n    bw:\n        Averaging works by rebinning\n        If auto, tries to work out the original bin spacing and then groups by this\n        If not auto, specify the bin width in distance.\n\n    \"\"\"\n    if coord is None:\n        if coord_i == \"all\":\n            coord_names = self.coord_cols\n        else:\n            coord_names = [self.coord_cols[coord_i]]\n    else:\n        coord_names = [coord]\n\n    if crease == 0.0 and not self.centred:\n        # if the crease is located at coord = 0, but not already centred then - centres\n        self.centre(coord=coord_names)\n\n    df = self.data.copy()\n\n    coord1 = df[coord_names[0]]\n\n    if bw == \"auto\":\n        bw = np.abs(\n            coord1.iloc[-1] - coord1.iloc[-2]\n        )  # if auto work out from the difference of the last 2 points of the coord\n\n    # # only index by the first coord,but flip all?\n    # select_a = (df[coord_names[0]] &gt;=  0)\n    # select_b = ~select_a\n\n    # df_a = df.loc[select_a].sort_values(by = coord_names[0] ,inplace = True,ignore_index = True)\n    # df_b = df.loc[select_b]\n    # df_b[coord_names]=df_b[coord_names].abs()\n    # df_b.sort_values(by = coord_names[0] ,inplace = True,ignore_index = True)\n\n    # df_ave = pd.concat({'a':df_a,'b':df_b}).mean(level=1)\n\n    # fold the df\n    df[coord_names] = (\n        np.abs(df[coord_names] - crease) + crease\n    )  # assumes already centred for now\n    df_ave = df_utils.rebin(df, coord_names[0], bw).sort_values(\n        coord_names[0]\n    )  # performing the rebinning # then sort by the coord\n\n    return df_ave\n</code></pre>"},{"location":"reference/chunk/#thermotar.chunk.Chunk.moment","title":"<code>moment(coord, weighting, order=1)</code>","text":"<p>Calculate the specified moment of the coordinate, weighted by a named property</p> Source code in <code>thermotar/chunk.py</code> <pre><code>def moment(\n    self,\n    coord,\n    weighting,\n    order=1,\n):\n    \"\"\"Calculate the specified moment of the coordinate, weighted by a named property\"\"\"\n    coords = self.data[self.choose_coordinate(coord)].T\n\n    integrand = self.data[weighting] * coords**order\n    normaliser = np.trapz(self.data[weighting], coords)\n\n    return np.trapz(integrand, coords) / normaliser\n</code></pre>"},{"location":"reference/chunk/#thermotar.chunk.Chunk.nearest","title":"<code>nearest(property, value, coord=0)</code>","text":"<p>Return the index for which property is closest to <code>value</code>.</p> <p>To do, return the actual row of properties??????</p> <p>This way if nan - make a nan list</p> Source code in <code>thermotar/chunk.py</code> <pre><code>def nearest(self, property: str, value: float, coord=0):\n    \"\"\"Return the index for which property is closest to `value`.\n\n    To do, return the actual row of properties??????\n\n    This way if nan - make a nan list\n\n    \"\"\"\n    # TODO: Find out where this method is used and understand what on earth it does\n    coord = self.data[\n        self.coord_cols[coord]\n    ]  # select coordinate column for which to find nearsest in each side\n\n    prop = self.data[property]\n    # if each half, find the index in each half of the simulation box\n\n    left = prop.loc[coord &lt; 0]\n    right = prop.loc[coord &gt; 0]\n\n    right_nearest_index = right.sub(value).abs().idxmin()\n\n    if len(left.index) != 0:\n        left_nearest_index = left.sub(value).abs().idxmin()\n        indexes = [left_nearest_index, right_nearest_index]\n    else:\n        indexes = [right_nearest_index]\n\n    try:\n        return self.data.loc[indexes].copy()\n    except KeyError:  # just create an empty row\n        row_copy = self.data.loc[\n            (0,), :\n        ].copy()  # janky indexing makes it a df rather than a series\n        row_copy.loc[:] = np.nan\n        return row_copy\n</code></pre>"},{"location":"reference/chunk/#thermotar.chunk.Chunk.parity","title":"<code>parity(prop, coord=0)</code>","text":"<p>Multiplies a property by the sign of the coordinate.</p> <p>Should only be applied to properties that are pseudo scalars,  i.e. change sign under coordinate inversion, so that upon folding and averaging properties are correct.</p> Source code in <code>thermotar/chunk.py</code> <pre><code>def parity(self, prop, coord=0):\n    \"\"\"\n    Multiplies a property by the sign of the coordinate.\n\n    Should only be applied to properties that are pseudo scalars,  i.e. change\n    sign under coordinate inversion, so that upon folding and averaging\n    properties are correct.\n    \"\"\"\n    # centre first\n    if isinstance(coord, int):\n        coord = self.coord_cols[coord]\n    if not self.centred:\n        self.centre()\n\n    self.data[prop + \"_original\"] = self.data[prop]\n    self.data[prop] *= np.sign(\n        self.data[coord]\n    )  # multiply by the sign of the coordinate column\n    self.raise_columns()\n</code></pre>"},{"location":"reference/chunk/#thermotar.chunk.Chunk.prop_grad","title":"<code>prop_grad(prop, coord, **kwargs)</code>","text":"<p>Calculate the gradient of <code>prop</code> with respect to <code>coord</code>.</p> <p>Parameters:</p> Name Type Description Default <code>prop</code> <code>str</code> <p>Which property the gradient of is calculated</p> required <code>coord</code> <code>str</code> <p>Which property is used for the coordinate.</p> required <code>kwargs</code> <p>Keyword arguments to pass to <code>np.gradient</code></p> <code>{}</code> Source code in <code>thermotar/chunk.py</code> <pre><code>def prop_grad(self, prop: str, coord: str, **kwargs):\n    \"\"\"Calculate the gradient of `prop` with respect to `coord`.\n\n    Parameters\n    ----------\n    prop:\n        Which property the gradient of is calculated\n    coord:\n        Which property is used for the coordinate.\n    kwargs:\n        Keyword arguments to pass to `np.gradient`\n    \"\"\"\n    df = self.data\n\n    df[prop + \"_grad\"] = np.gradient(df[prop], df[coord], **kwargs)\n\n    # updates the columns\n    df_utils.raise_columns(self)\n</code></pre>"},{"location":"reference/chunk/#thermotar.chunk.Chunk.raise_columns","title":"<code>raise_columns()</code>","text":"<p>Raise columns from the df to be attributes.</p> Source code in <code>thermotar/chunk.py</code> <pre><code>def raise_columns(self):\n    \"\"\"Raise columns from the df to be attributes.\"\"\"\n    # I have no clue how pandas does this automatically...\n    # Maybe I need to make it so my objects can be indexed\n    # and in doing that for assignment, the attributes can be raised up\n    # TODO : Something with above??\n    df_utils.raise_columns(self)\n</code></pre>"},{"location":"reference/chunk/#thermotar.chunk.Chunk.rebin","title":"<code>rebin(coord, bw=0.25, bins=None, nbins=None, mode='average', inplace=False, new_coord_loc='mid', weights=None)</code>","text":"<p>Rebin the data based on coordinates for a given new bin width.</p> <p>Default is to perform averages over these bins. Could also add weightings for these averages</p> <p>Parameters:</p> Name Type Description Default <code>coord</code> <p>Column name of the coordinate to create the bins from</p> required <code>nbins</code> <p>None, int, or array of ints. Number of bins for each binning dimension. Currently only supported for 2D bins.</p> <code>None</code> <code>new_coord_loc</code> <p>\"mid\" (default), \"left\" or \"right\" position of new coordinate, when set manually rather than from average. Currently only for 2D bins</p> <code>'mid'</code> <code>inplace</code> <code>bool</code> <p>if True, overwrites the .data method, else just returns the data frame.</p> <code>False</code> <code>weights</code> <p>Column label for performing a weighted average, only used if mode is \"average\" or \"mean\"</p> <code>None</code> Source code in <code>thermotar/chunk.py</code> <pre><code>def rebin(\n    self,\n    coord,\n    bw=0.25,\n    bins=None,\n    nbins=None,\n    mode=\"average\",\n    inplace=False,\n    new_coord_loc=\"mid\",\n    weights=None,\n):\n    \"\"\"\n    Rebin the data based on coordinates for a given new bin width.\n\n    Default is to perform averages over these bins.\n    Could also add weightings for these averages\n\n    Parameters\n    ----------\n    coord:\n        Column name of the coordinate to create the bins from\n\n    nbins:\n        None, int, or array of ints. Number of bins for each binning dimension.\n        Currently only supported for 2D bins.\n\n    new_coord_loc:\n        \"mid\" (default), \"left\" or \"right\" position of new coordinate,\n        when set manually rather than from average. Currently only for 2D bins\n\n    inplace : bool\n        if True, overwrites the .data method, else just returns the data frame.\n\n    weights:\n        Column label for performing a weighted average,\n        only used if mode is \"average\" or \"mean\"\n\n    \"\"\"\n    # TODO: - implement n_bins argument for 1d bins\n    df = self.data\n\n    # Use multiple bins\n    if np.iterable(coord) and not isinstance(coord, str):\n        number_coords = len(coord)\n        if number_coords == 2:\n            coord1, coord2 = coord[0], coord[1]\n            df_binned = df_utils.rebin_2D(\n                df,\n                coord1,\n                coord2,\n                bw=bw,\n                bins=bins,\n                mode=mode,\n                nbins=nbins,\n                new_coord_loc=new_coord_loc,\n                weight_col=weights,\n            )\n        elif np.number &gt;= 3:\n            raise NotImplementedError(\n                \"Binning in more than two dimensions not yet supported\"\n            )\n    else:\n        df_binned = df_utils.rebin(\n            df, coord, bw=bw, mode=mode, weight_col=weights, bins=bins\n        )\n    # coord_max = df[coord].max()\n    # coord_min = df[coord].min() # finding min and max so it works with gfolded data\n\n    # n_bins = (coord_max-coord_min) // bw #double divide is floor division\n    # n_bins = int(n_bins)\n\n    # bins = pd.cut(df[coord],n_bins)\n\n    # df_grouped = df.groupby(bins)\n\n    # if mode == 'average' or mode == 'mean':\n    #     df_binned = df_grouped.mean()\n    # else:\n    #     df_binned = df_grouped.sum()\n\n    # TODO:  Why is df_binned possibly unbound?\n    if inplace:\n        self.data = df_binned\n    else:\n        return df_binned\n</code></pre>"},{"location":"reference/chunk/#multichunk","title":"MultiChunk","text":"<p>An extended Version of Chunk </p> Source code in <code>thermotar/multichunk.py</code> <pre><code>class MultiChunk:\n    def __init__(\n        self,\n        df,\n        file2=None,\n        CLEANUP=True,\n        coord_cols=[\"Coord1\", \"Coord2\", \"Coord3\", \"coord\"],\n        centred=False,\n        centered=None,\n        **kwargs,\n    ):\n        \"\"\"thermo_file - string of log file location\"\"\"\n        self.data: pd.DataFrame = df\n\n        # clean up dataframe\n\n        # apply strip_pref function to remove 'c_/f_/v_' prefixes to all columns\n        if CLEANUP:\n            self.data.rename(columns=lmp_utils.strip_pref, inplace=True)\n            self.data.rename(columns=lmp_utils.drop_python_bad, inplace=True)\n            # todo merge columns into vectors\n\n        # set the columns as attributes\n        for col in self.data.columns:\n            # setattr(self, col ,getattr(self.data, col))\n            # has to be set to a method of the class\n            setattr(\n                self.__class__, col, df_utils.raise_col(self, col)\n            )  # set attribute to this property object??\n        # column names for the coordinates, up to 3\n        # only those in the df are included, by finding intersect of sets.\n        self.coord_cols = list(set(self.data.columns.to_list()) &amp; set(coord_cols))\n        if centered is not None:\n            centred = centered\n\n        self.centred = centred  # Initialise assuming asymmetrical - to do implement a method to check this!!!!\n\n    def copy(self):\n        new_chunk = MultiChunk(self.data.copy())\n        new_chunk.coord_cols = self.coord_cols\n        new_chunk.centred = self.centred\n        return new_chunk\n\n    def zero_to_nan(self, val=0.0, col=None):\n        \"\"\"\n        Replace an exact value specified by val with nan\n        In columns col\n        Improves averages\n        \"\"\"\n        if col is None:\n            to_replace = {val: np.nan}\n            value = np.nan\n            self.data = self.data.replace(to_replace=to_replace)\n            return\n            # self.data = self.data.replace(to_replace=)\n        elif isinstance(col, list):\n            to_replace = {column: val for column in col}\n            value = np.nan\n        else:\n            to_replace = {col: val}\n            value = np.nan\n        self.data = self.data.replace(to_replace=to_replace, value=value)\n\n    def zero_to_nan_return(self, val=0.0, col=None):\n        \"\"\"\n        Replace an exact value specified by val with nan\n        In columns col\n        Improves averages\n        Returns a new MultiChunk\n        \"\"\"\n        new_chunk = self.copy()\n        new_chunk.zero_to_nan(val=val, col=col)\n        return new_chunk\n\n    def thresh_to_nan_inplace(\n        self,\n        col,\n        thresh=0.0,\n    ):\n        \"\"\"\n        Replace values below a threshold with Nan\n        In columns col\n        Improves averages\n\n        TODO actually implement what this says, not just removing the rows.....\n        \"\"\"\n        df = self.data\n\n        self.data = self.data.loc[df[col] &gt;= thresh]\n\n    def thersh_to_nan_return(self, col, thresh=0.0):\n        \"\"\"\n        Replace values below a threshold with Nan\n        In columns col(ums)\n        Improves averages\n        Returns a new object\n        \"\"\"\n        # TODO use views or something instead, idk\n        # TODO create a clone method for chunk/multichunk/ for the the \"DataHolder\" class...\n        new_chunk = MultiChunk(self.data.copy())\n        new_chunk.thresh_to_nan_inplace(col=col, thresh=thresh)\n\n        return new_chunk\n\n    def zero_to_nan_return(self, val=0.0, col=None):\n        \"\"\"\n        Replace an exact value specified by val with nan\n        In columns col\n        Improves averages\n        Same as zero_to_nan but creates a new obj\n        \"\"\"\n        if col is None:\n            # apply to all cols\n            return MultiChunk(self.data.replace(to_replace={val: np.nan}).copy())\n        else:\n            return MultiChunk(\n                self.data.replace(to_replace={col: val}, value=np.nan).copy()\n            )\n\n    def flatten_chunk(self, drop_na=None) -&gt; Chunk:\n        df = self.data.groupby(level=(3)).mean()\n\n        if drop_na is not None:\n            df.dropna(how=drop_na, inplace=True)\n\n        return Chunk(df)\n\n    @staticmethod\n    def create_multi_chunks(fname, *, verbose=False, **read_csv_kwargs):\n        parser: LMPChunksParser = parse_lmp_chunks(\n            fname, verbose=verbose, **read_csv_kwargs\n        )\n\n        if verbose:\n            print(\"Smallest chunk:\", np.min(parser.n_chunks))\n            print(\"Biggest chunk:\", np.max(parser.n_chunks))\n\n        return MultiChunk(parser.data)\n</code></pre>"},{"location":"reference/chunk/#thermotar.multichunk.MultiChunk.__init__","title":"<code>__init__(df, file2=None, CLEANUP=True, coord_cols=['Coord1', 'Coord2', 'Coord3', 'coord'], centred=False, centered=None, **kwargs)</code>","text":"<p>thermo_file - string of log file location</p> Source code in <code>thermotar/multichunk.py</code> <pre><code>def __init__(\n    self,\n    df,\n    file2=None,\n    CLEANUP=True,\n    coord_cols=[\"Coord1\", \"Coord2\", \"Coord3\", \"coord\"],\n    centred=False,\n    centered=None,\n    **kwargs,\n):\n    \"\"\"thermo_file - string of log file location\"\"\"\n    self.data: pd.DataFrame = df\n\n    # clean up dataframe\n\n    # apply strip_pref function to remove 'c_/f_/v_' prefixes to all columns\n    if CLEANUP:\n        self.data.rename(columns=lmp_utils.strip_pref, inplace=True)\n        self.data.rename(columns=lmp_utils.drop_python_bad, inplace=True)\n        # todo merge columns into vectors\n\n    # set the columns as attributes\n    for col in self.data.columns:\n        # setattr(self, col ,getattr(self.data, col))\n        # has to be set to a method of the class\n        setattr(\n            self.__class__, col, df_utils.raise_col(self, col)\n        )  # set attribute to this property object??\n    # column names for the coordinates, up to 3\n    # only those in the df are included, by finding intersect of sets.\n    self.coord_cols = list(set(self.data.columns.to_list()) &amp; set(coord_cols))\n    if centered is not None:\n        centred = centered\n\n    self.centred = centred  # Initialise assuming asymmetrical - to do implement a method to check this!!!!\n</code></pre>"},{"location":"reference/chunk/#thermotar.multichunk.MultiChunk.thersh_to_nan_return","title":"<code>thersh_to_nan_return(col, thresh=0.0)</code>","text":"<p>Replace values below a threshold with Nan In columns col(ums) Improves averages Returns a new object</p> Source code in <code>thermotar/multichunk.py</code> <pre><code>def thersh_to_nan_return(self, col, thresh=0.0):\n    \"\"\"\n    Replace values below a threshold with Nan\n    In columns col(ums)\n    Improves averages\n    Returns a new object\n    \"\"\"\n    # TODO use views or something instead, idk\n    # TODO create a clone method for chunk/multichunk/ for the the \"DataHolder\" class...\n    new_chunk = MultiChunk(self.data.copy())\n    new_chunk.thresh_to_nan_inplace(col=col, thresh=thresh)\n\n    return new_chunk\n</code></pre>"},{"location":"reference/chunk/#thermotar.multichunk.MultiChunk.thresh_to_nan_inplace","title":"<code>thresh_to_nan_inplace(col, thresh=0.0)</code>","text":"<p>Replace values below a threshold with Nan In columns col Improves averages</p> <p>TODO actually implement what this says, not just removing the rows.....</p> Source code in <code>thermotar/multichunk.py</code> <pre><code>def thresh_to_nan_inplace(\n    self,\n    col,\n    thresh=0.0,\n):\n    \"\"\"\n    Replace values below a threshold with Nan\n    In columns col\n    Improves averages\n\n    TODO actually implement what this says, not just removing the rows.....\n    \"\"\"\n    df = self.data\n\n    self.data = self.data.loc[df[col] &gt;= thresh]\n</code></pre>"},{"location":"reference/chunk/#thermotar.multichunk.MultiChunk.zero_to_nan","title":"<code>zero_to_nan(val=0.0, col=None)</code>","text":"<p>Replace an exact value specified by val with nan In columns col Improves averages</p> Source code in <code>thermotar/multichunk.py</code> <pre><code>def zero_to_nan(self, val=0.0, col=None):\n    \"\"\"\n    Replace an exact value specified by val with nan\n    In columns col\n    Improves averages\n    \"\"\"\n    if col is None:\n        to_replace = {val: np.nan}\n        value = np.nan\n        self.data = self.data.replace(to_replace=to_replace)\n        return\n        # self.data = self.data.replace(to_replace=)\n    elif isinstance(col, list):\n        to_replace = {column: val for column in col}\n        value = np.nan\n    else:\n        to_replace = {col: val}\n        value = np.nan\n    self.data = self.data.replace(to_replace=to_replace, value=value)\n</code></pre>"},{"location":"reference/chunk/#thermotar.multichunk.MultiChunk.zero_to_nan_return","title":"<code>zero_to_nan_return(val=0.0, col=None)</code>","text":"<p>Replace an exact value specified by val with nan In columns col Improves averages Same as zero_to_nan but creates a new obj</p> Source code in <code>thermotar/multichunk.py</code> <pre><code>def zero_to_nan_return(self, val=0.0, col=None):\n    \"\"\"\n    Replace an exact value specified by val with nan\n    In columns col\n    Improves averages\n    Same as zero_to_nan but creates a new obj\n    \"\"\"\n    if col is None:\n        # apply to all cols\n        return MultiChunk(self.data.replace(to_replace={val: np.nan}).copy())\n    else:\n        return MultiChunk(\n            self.data.replace(to_replace={col: val}, value=np.nan).copy()\n        )\n</code></pre>"},{"location":"reference/interface/","title":"Intersect and Interface Finding","text":""},{"location":"reference/interface/#example","title":"Example","text":"<p>This module contains a class <code>InterfaceFinder</code> for finding the interface between two or more fluids. This is achieved by finding the location where the two densities are equal.</p> <p>To ensure that no false interfaces are found, the dataset should mask the points where the density is exactly zero.</p> <p>All data frames must also have the exact same coordinate grid (i.e. same start, end and spacing).</p> <pre><code>import thermotar as th\nfrom thermotar.intersects import InterfaceFinder, multi_intersect, PairIntersectFinders\n\ndef _is_zero_density(df):\n    \"\"\"\n    Helper function that masking zero density.\n\n    Using a helper allows us to load and mask in one line easily.\n    \"\"\"\n    return df[\"density_number\"] == 0.0\n\n\ndef _is_left(df):\n    return df[\"Coord1\"] &lt; 0.0\n\n# Load in the data for each section\n# and mask where the densities are zero\n# Masking ensures intersection isn't found where both have zero density\n# Also masking the left interface, can only do one at a time.\ndf_au = (\n    th.create_chunk(\"./resources/gold.chunk\").data.mask(_is_zero_density).mask(_is_left)\n)\ndf_ligand = (\n    th.create_chunk(\"./resources/ligand.chunk\")\n    .data.mask(_is_zero_density)\n    .mask(_is_left)\n)\n# For simplicity just using oxygen atoms.\n# Ideally should use oxygen + hydrogen.\ndf_water = (\n    th.create_chunk(\"./resources/oxygen.chunk\")\n    .data.mask(_is_zero_density)\n    .mask(_is_left)\n)\n</code></pre> <p>The <code>InterfaceFinder</code> class takes in a list of DataFrames to find interfaces between and the label of the spatial coordinate.  This list of DataFrames must be in the order that the components appear spatially.  For components A, B and C, must be in the order <code>[df_A,df_B,df_C]</code>, if you want to find the interfaces between A and B and B and C.</p> <pre><code># Intersections\n# Data frames must be in spatial order.\ninter_finder = InterfaceFinder(\n    [df_au, df_ligand, df_water], \"Coord1\", y_coord=\"density_number\"\n)\n</code></pre> <p>The constructor automatically finds the intersects and creates masked DataFrames.</p> <p>There are a variety of attributes and convinence methods,</p> <pre><code># Locations of the intersects\nprint(inter_finder.intersects)\n\n# Masked Dataframes\nprint(inter_finder.masked_dfs)\n\n# A method for plotting the data\n# These plots aren't very pretty, but good for debugging\ninter_finder.make_plots([\"density_number\", \"temp\"], show_original=True)\nplt.show()\n\n# Extrapolate the temperatures to the interfaces\nprint(inter_finder.interface_values(y_props=\"temp\"))\n\n# Estimate the temperature jumps at the interfaces\nprint(inter_finder.deltas(y_props=\"temp\"))\n</code></pre> <p>If you don't care about all these methods and just want the raw interface locations, you can just use <code>multi_intersect</code>, which is what is used internally by <code>InterfaceFinder</code></p> <pre><code># You can alternatively just get the interface locations using\n# note, the y_column is required now\nintersects = multi_intersect([df_au, df_ligand, df_water], \"Coord1\", \"density_number\")\n</code></pre> <p>Or if you only have the one interface, you may prefer the pair methods used internally by multi intersect <pre><code># And if you only care about one interface, you can use the `PairIntersect`s directly\nPairIntersectFinders.interp(df_ligand, df_water, \"Coord1\", \"density_number\")\n</code></pre></p>"},{"location":"reference/interface/#reference","title":"Reference","text":""},{"location":"reference/interface/#thermotar.intersects.InterfaceFinder","title":"<code>InterfaceFinder</code>  <code>dataclass</code>","text":"<p>Find the interfaces between several DataFrames</p> <p>Create using: <pre><code>finder = InterfaceFinder(dataframes, x_coord, y_coord)\n</code></pre> Access the interface locations using: <pre><code>finder.intersects\n</code></pre> Extrapolate the properties to the interface using: <pre><code>finder.interface_values()\n</code></pre> Attributes:     Required:     dataframes: List of DataFrames to find interfaces between     x_coord: Name of the x coordinate column</p> <pre><code>Optional:\ny_coord=\"density_number\": Name of the y coordinate column to find the intersects between\n\nintersect_method: Method to use to find the intersects. See `multi_intersect` for options\nintersect_kwargs: Keyword arguments to pass to the intersect method\npad_masks: Amount to pad the masks by. If None, no padding is applied\nintersects: List of the intersect locations, calculated in `__post_init__`\nbounds: List of the bounds of the masks, calculated in `__post_init__`\nmasks: List of the masks, calculated in `__post_init__`\nmasked_dfs: List of the masked DataFrames, calculated in `__post_init__`\n</code></pre> Source code in <code>thermotar/intersects.py</code> <pre><code>@dataclass\nclass InterfaceFinder:\n    \"\"\"\n    Find the interfaces between several DataFrames\n\n    Create using:\n    ```python\n    finder = InterfaceFinder(dataframes, x_coord, y_coord)\n    ```\n    Access the interface locations using:\n    ```python\n    finder.intersects\n    ```\n    Extrapolate the properties to the interface using:\n    ```python\n    finder.interface_values()\n    ```\n    Attributes:\n        Required:\n        dataframes: List of DataFrames to find interfaces between\n        x_coord: Name of the x coordinate column\n\n        Optional:\n        y_coord=\"density_number\": Name of the y coordinate column to find the intersects between\n\n        intersect_method: Method to use to find the intersects. See `multi_intersect` for options\n        intersect_kwargs: Keyword arguments to pass to the intersect method\n        pad_masks: Amount to pad the masks by. If None, no padding is applied\n        intersects: List of the intersect locations, calculated in `__post_init__`\n        bounds: List of the bounds of the masks, calculated in `__post_init__`\n        masks: List of the masks, calculated in `__post_init__`\n        masked_dfs: List of the masked DataFrames, calculated in `__post_init__`\n\n    \"\"\"\n\n    dataframes: List[pd.DataFrame]\n\n    x_coord: str\n    y_coord: str = \"density_number\"\n\n    intersect_method: str = \"linear\"\n    intersect_kwargs: Dict = field(default_factory=dict)\n    pad_masks: float = None\n\n    intersects: List[float] = field(init=False)\n    bounds: List[Tuple[float]] = field(init=False)\n    masks: List[pd.DataFrame] = field(init=False)\n    masked_dfs: List[pd.DataFrame] = field(init=False)\n\n    def __post_init__(self):\n        self.intersects = multi_intersect(\n            self.dataframes,\n            self.x_coord,\n            self.y_coord,\n            intersect_method=self.intersect_method,\n            **self.intersect_kwargs,\n        )\n        self.bounds = bounds_from_intersects(self.intersects)\n        self.masks = masks_from_intersects(\n            self.dataframes, self.intersects, self.x_coord, padding=self.pad_masks\n        )\n        self.masked_dfs = apply_masks(self.dataframes, self.masks)\n\n    def interface_values(\n        self,\n        fit_range=5,\n        y_props=None,\n        n=1,\n        return_fits=False,\n        group_by_interface=False,\n        error_suffix=None,\n    ):\n        \"\"\"\n        Find the values extrapolated to the interfaces\n        \"\"\"\n        return interface_values(\n            self.masked_dfs,\n            self.bounds,\n            x_prop=self.x_coord,\n            fit_range=fit_range,\n            y_props=y_props,\n            n=n,\n            return_fits=return_fits,\n            group_by_interface=group_by_interface,\n            error_suffix=error_suffix,\n        )\n\n    def deltas(self, **interface_val_kwargs):\n        # dfs = self.interface_values(**interface_val_kwargs)\n\n        # df = pd.concat(dfs)\n        # df.groupby(by = self.x_coord)\n\n        deltas = {}\n\n        for i, x in enumerate(self.intersects):\n            y_a = extrapolate_to_interface(\n                self.masked_dfs[i],\n                x,\n                self.x_coord,\n                return_coeffs=False,\n                **interface_val_kwargs,\n            )\n            y_b = extrapolate_to_interface(\n                self.masked_dfs[i + 1],\n                x,\n                self.x_coord,\n                return_coeffs=False,\n                **interface_val_kwargs,\n            )\n\n            deltas[x] = y_a - y_b\n\n        return pd.concat(deltas).droplevel(1)\n\n    def make_plots(\n        self,\n        y_props,\n        axs=None,\n        show_extrap=False,\n        show_original=False,\n        extrap_options={\"fit_range\": 5, \"n\": 1},\n        colours=None,\n        **kwargs,\n    ):\n        import matplotlib.colors as mcolors\n\n        if colours is None:\n            colours = [c for c in mcolors.BASE_COLORS.values()]\n        x_prop = self.x_coord\n        dfs = self.masked_dfs\n        og_dfs = self.dataframes\n        x_inter = self.intersects\n        if not show_extrap:\n            interface_vals = self.interface_values(\n                return_fits=show_extrap, y_props=y_props, **extrap_options\n            )\n        else:\n            interface_vals, fits = self.interface_values(\n                return_fits=show_extrap, y_props=y_props, **extrap_options\n            )\n        interface_vals_group = self.interface_values(\n            group_by_interface=True, y_props=y_props, **extrap_options\n        )\n\n        fig, axs = plt.subplots(len(y_props), sharex=False)\n\n        for ax, y_prop in zip(axs, y_props):\n            for j, (df, og_df) in enumerate(zip(dfs, og_dfs)):\n                # Plot the data\n                if not show_original:\n                    og_df = None\n                make_plot(\n                    df, x_prop, y_prop, ax, colour=colours[j], og_df=og_df, **kwargs\n                )\n                if show_extrap:\n                    for k, x0 in enumerate(\n                        (b for b in self.bounds[j] if b is not None)\n                    ):\n                        if x0 is not None:\n                            xs = np.linspace(\n                                x0 - extrap_options[\"fit_range\"],\n                                x0 + extrap_options[\"fit_range\"],\n                            )\n                            plot_extrap(\n                                df, y_prop, xs, fits[j][k], ax, colour=colours[j]\n                            )\n\n                x = interface_vals[j][x_prop]\n                y = interface_vals[j][y_prop]\n                ax.plot(x, y, marker=\".\", ls=\" \", c=colours[j], mec=\"k\")\n            # plot the temperature jumps\n            if y_prop != \"density_number\":\n                for x, df in interface_vals_group.groupby(level=0):\n                    y = df[y_prop]\n                    ax.annotate(\n                        \"\",\n                        xy=(x, y.min()),\n                        xytext=(x, y.max()),\n                        xycoords=\"data\",\n                        textcoords=\"data\",\n                        arrowprops=dict(\n                            arrowstyle=\"&lt;-&gt;\", connectionstyle=\"arc3\", color=\"k\", lw=1\n                        ),\n                    )\n\n            for x in x_inter:\n                ax.axvline(x, ls=\"--\", c=\"grey\")\n</code></pre>"},{"location":"reference/interface/#thermotar.intersects.InterfaceFinder.interface_values","title":"<code>interface_values(fit_range=5, y_props=None, n=1, return_fits=False, group_by_interface=False, error_suffix=None)</code>","text":"<p>Find the values extrapolated to the interfaces</p> Source code in <code>thermotar/intersects.py</code> <pre><code>def interface_values(\n    self,\n    fit_range=5,\n    y_props=None,\n    n=1,\n    return_fits=False,\n    group_by_interface=False,\n    error_suffix=None,\n):\n    \"\"\"\n    Find the values extrapolated to the interfaces\n    \"\"\"\n    return interface_values(\n        self.masked_dfs,\n        self.bounds,\n        x_prop=self.x_coord,\n        fit_range=fit_range,\n        y_props=y_props,\n        n=n,\n        return_fits=return_fits,\n        group_by_interface=group_by_interface,\n        error_suffix=error_suffix,\n    )\n</code></pre>"},{"location":"reference/interface/#thermotar.intersects.PairIntersectFinders","title":"<code>PairIntersectFinders</code>","text":"<p>Namespace collection of methods for find the intersects between DataFrames</p> <pre><code>`coarse`:   Finds the interface crudely. Finds the index of the row with the\n            smallest absolute difference between the y_property\n`interp`:   Interpolates the difference in the y property first, then uses fsolve\n            to find a more precise intersection. Uses `coarse` for a first guess.\n</code></pre> Source code in <code>thermotar/intersects.py</code> <pre><code>class PairIntersectFinders:\n    \"\"\"\n    Namespace collection of methods for find the intersects between DataFrames\n\n        `coarse`:   Finds the interface crudely. Finds the index of the row with the\n                    smallest absolute difference between the y_property\n        `interp`:   Interpolates the difference in the y property first, then uses fsolve\n                    to find a more precise intersection. Uses `coarse` for a first guess.\n\n\n    \"\"\"\n\n    @staticmethod\n    def coarse(df_a: pd.DataFrame, df_b: pd.DataFrame, x_prop, y_prop) -&gt; float:\n        \"\"\"\n        Find the intersect between a pair of DataFrames.\n        This uses a crude approximation, it finds the index in both DataFrames where\n        df_a[y_prop]-df_b[y_prop] is at it's smallest absolute value and then returns the\n        x_prop value at this index.\n\n        \"\"\"\n\n        index_cross = (df_a[y_prop] - df_b[y_prop]).abs().idxmin()\n\n        x_a = df_a[x_prop].loc[index_cross]\n        x_b = df_b[x_prop].loc[index_cross]\n\n        if ~np.isclose(x_a, x_b):\n            raise ValueError(\"Coordinates of bins different!\")\n\n        return x_a\n\n    @staticmethod\n    def interp(df_a: pd.DataFrame, df_b: pd.DataFrame, x_prop, y_prop, kind=\"linear\"):\n        x_a, y_a = df_a[x_prop], df_a[y_prop]\n        x_b, y_b = df_b[x_prop], df_b[y_prop]\n        if (x_a != x_b).all():\n            raise ValueError(\"Grids not the same!\")\n\n        # interp1d(x_a, y_a, kind=kind)\n        # interp1d(x_b, y_b, kind=kind)\n        f_delta = interp1d(x_a, y_b - y_a, kind=kind, fill_value=\"extrapolate\")\n        # f_grad = interp1d(x_a,np.gradient(y_b-y_a,x_a),kind=kind) # interpolation of gradient\n        # estimate from coarse!\n        x_0 = PairIntersectFinders.coarse(df_a, df_b, x_prop, y_prop)\n        dx = x_a.diff().mean()\n        intersect = fsolve(\n            f_delta,\n            x0=x_0 - dx / 2,\n        )\n\n        if len(intersect) &gt; 1:\n            print(\"WARNING: Multiple intersects found\")\n            return intersect[0]\n        elif len(intersect) == 1:\n            return intersect[0]\n        else:\n            raise NotImplementedError(\"No intersect found\")\n\n    @staticmethod\n    def spline(\n        df_a: pd.DataFrame,\n        df_b: pd.DataFrame,\n        x_prop,\n        y_prop,\n        y_err=None,\n        show_plots=False,\n        s=0.5,\n    ):\n        x_a, y_a = df_a[x_prop], df_a[y_prop]\n        x_b, y_b = df_b[x_prop], df_b[y_prop]\n        if (x_a != x_b).all():\n            raise ValueError(\"Grids not the same!\")\n        delta_y = y_a - y_b\n        delta_not_na = ~delta_y.isna()\n        delta_y = delta_y[delta_not_na]\n        x = x_a[delta_not_na]\n\n        if y_err is not None:\n            y_err = 1 / (df_a[y_err] + df_b[y_err])[delta_not_na]  # add errors\n\n        spline_delta = UnivariateSpline(x, delta_y, w=y_err, s=s)\n        intersects = spline_delta.roots()\n        if show_plots:\n            print(intersects)\n            plt.errorbar(x, delta_y, yerr=y_err, fmt=\"k.\")\n            plt.plot(x, spline_delta(x), c=\"blue\", ls=\"--\")\n            for root in intersects:\n                plt.axvline(root, ls=\"--\", c=\"grey\")\n        if len(intersects) == 1:\n            return intersects[0]\n        else:\n            raise NotImplementedError(\n                \"TODO: Raise warning if more than one intersect is found!\"\n            )\n</code></pre>"},{"location":"reference/interface/#thermotar.intersects.PairIntersectFinders.coarse","title":"<code>coarse(df_a, df_b, x_prop, y_prop)</code>  <code>staticmethod</code>","text":"<p>Find the intersect between a pair of DataFrames. This uses a crude approximation, it finds the index in both DataFrames where df_a[y_prop]-df_b[y_prop] is at it's smallest absolute value and then returns the x_prop value at this index.</p> Source code in <code>thermotar/intersects.py</code> <pre><code>@staticmethod\ndef coarse(df_a: pd.DataFrame, df_b: pd.DataFrame, x_prop, y_prop) -&gt; float:\n    \"\"\"\n    Find the intersect between a pair of DataFrames.\n    This uses a crude approximation, it finds the index in both DataFrames where\n    df_a[y_prop]-df_b[y_prop] is at it's smallest absolute value and then returns the\n    x_prop value at this index.\n\n    \"\"\"\n\n    index_cross = (df_a[y_prop] - df_b[y_prop]).abs().idxmin()\n\n    x_a = df_a[x_prop].loc[index_cross]\n    x_b = df_b[x_prop].loc[index_cross]\n\n    if ~np.isclose(x_a, x_b):\n        raise ValueError(\"Coordinates of bins different!\")\n\n    return x_a\n</code></pre>"},{"location":"reference/interface/#thermotar.intersects.bounds_from_intersects","title":"<code>bounds_from_intersects(intersects)</code>","text":"<p>Find the limits either side of each dataframe. TODO: Check that the functions that call this have the right signature</p> Source code in <code>thermotar/intersects.py</code> <pre><code>def bounds_from_intersects(\n    intersects: List[float],\n):\n    \"\"\"\n    Find the limits either side of each dataframe.\n    TODO: Check that the functions that call this have the right signature\n    \"\"\"\n    # N = len(dfs)\n    # if N != len(intersects) + 1:\n    #     raise ValueError(\"Must always be N-1 intersects!\")\n    N = len(intersects) + 1\n    bounds = []\n\n    for i in range(N):\n        if i == 0:\n            bounds.append((None, intersects[0]))\n        elif i == N - 1:\n            bounds.append((intersects[i - 1], None))\n        else:\n            bounds.append((intersects[i - 1], intersects[i]))\n    if N != len(bounds):\n        raise ValueError(\"There was not one set of bounds per DataFrame!\")\n    return bounds\n</code></pre>"},{"location":"reference/interface/#thermotar.intersects.extrapolate_to_interface","title":"<code>extrapolate_to_interface(df_a, x_intersect, x_prop, fit_range=5, return_coeffs=True, return_fits=False, y_props=None, error_suffix=None, n=1)</code>","text":"<p>Return a row extrapolated with a polynomial of order <code>n</code> to the interface, for a single DataFrame.</p> <p>df_a: pd.DataFrame = DataFrame to extrapolate to interface fit_range: float = Range either side of the interface to fit to. x_intersect: float = x coordinate of the interface x_prop: str = Name of x coordinate y_props: List[str] = List of columns to fit to. If None, fit to all columns.</p> <p>error_suffix : str = Suffix of error column to use for weighting. If None, no weighting is used.                      TODO: Currently not working correctly</p> <p>returns a dataframe row with the order TODO: add weightings by errors TODO: Neaten weighting by errors code to handle missing columns. TODO: Weighting can only currently be used on columns when not None</p> Source code in <code>thermotar/intersects.py</code> <pre><code>def extrapolate_to_interface(\n    df_a: pd.DataFrame,\n    x_intersect: float,\n    x_prop: str,\n    fit_range=5,\n    return_coeffs=True,\n    return_fits=False,\n    y_props=None,\n    error_suffix=None,\n    n=1,\n) -&gt; Union[pd.DataFrame, Tuple[pd.DataFrame, np.ndarray]]:\n    \"\"\"\n    Return a row extrapolated with a polynomial of order `n` to the interface, for a single DataFrame.\n\n    df_a: pd.DataFrame = DataFrame to extrapolate to interface\n    fit_range: float = Range either side of the interface to fit to.\n    x_intersect: float = x coordinate of the interface\n    x_prop: str = Name of x coordinate\n    y_props: List[str] = List of columns to fit to. If None, fit to all columns.\n\n    error_suffix : str = Suffix of error column to use for weighting. If None, no weighting is used.\n                         TODO: Currently not working correctly\n\n\n    returns a dataframe row with the order\n    TODO: add weightings by errors\n    TODO: Neaten weighting by errors code to handle missing columns.\n    TODO: Weighting can only currently be used on columns when not None\n    \"\"\"\n\n    df = df_a.query(f\"{x_intersect-fit_range}&lt;={x_prop}&lt;={x_intersect+fit_range}\")\n\n    x = df[x_prop]\n    # If intersect is a data point, return row with data in.\n    if (x == x_intersect).any():\n        return df[x == x_intersect]\n\n    if y_props is None and error_suffix is not None:\n        raise ValueError(\n            \"Cannot currently use weighted fitting without specifying columns\"\n        )\n\n    print(y_props)\n\n    if y_props is None:\n        # Use all columns excluding x_prop and then readd\n        y_props = list(df.drop(columns=x_prop).columns)\n        # y = df #.drop(columns=x_prop) # Include x prop in fit to make life easier\n    elif isinstance(y_props, str):\n        y_props = [y_props]\n        # Turn string into a list, so a DataFrame not a series is returned\n        # y = df[[y_props]]\n    y_props = y_props.copy()\n    y_props.insert(0, x_prop)  # put x at the beginning of the list\n    y = df[y_props]\n\n    if error_suffix is None:\n        # Use all columns\n        weights = (\n            None  # .drop(columns=x_prop) # Include x prop in fit to make life easier\n        )\n    else:\n        # y_errs = [label+error_suffix for label in y_props]\n        # weights = [1/df[y_errs]  ]\n        all_cols = df.columns\n        # Iterate over columns. If in all_cols set to 1/error, else just None\n        weights = {\n            err_col: 1 / df[err_col]\n            if err_col in all_cols and err_col != x_prop\n            else None\n            for err_col in (col + error_suffix for col in y_props)\n        }\n\n    # Create polynomial fit object for each column\n    if weights is None:\n        # Unweighted fit for each column\n        polys = {\n            col: Polynomial.fit(x, y[col], n, w=None) for col in y_props\n        }  # np.polynomial.polynomial.polyfit(x,y,deg=1)  #[::-1]\n    else:\n        # Weighted fit for each column\n        polys = {\n            col: Polynomial.fit(x, y[col], n, w=weights[col + error_suffix])\n            for col in y_props\n        }  # np.polynomial.polynomial.polyfit(x,y,deg=1)  #[::-1]\n    # Find the value at the interface\n    y_pred = {col: [poly(x_intersect)] for col, poly in polys.items()}\n    # Convert prediction int DataFrame row\n    df_pred = pd.DataFrame(y_pred)  # .reshape((1,-1)),columns=df.columns)\n\n    # Return prediction and fit objects\n    if return_fits:\n        return df_pred, polys\n    # Return prediction and coefficients\n    elif return_coeffs:\n        coeffs = {col: poly.coef for col, poly in polys.items()}\n        return df_pred, coeffs  # fit\n    # Just return prediction\n    else:\n        return df_pred\n</code></pre>"},{"location":"reference/interface/#thermotar.intersects.interface_values","title":"<code>interface_values(dfs, bounds, x_prop, fit_range=5, y_props=None, n=1, return_fits=False, group_by_interface=False, error_suffix=None)</code>","text":"<p>For all DataFrames in <code>dfs</code>, extrapolate to the interface defined by <code>bounds</code>.</p> Source code in <code>thermotar/intersects.py</code> <pre><code>def interface_values(\n    dfs: List[pd.DataFrame],\n    bounds,\n    x_prop,\n    fit_range=5,\n    y_props=None,\n    n=1,\n    return_fits=False,\n    group_by_interface=False,\n    error_suffix=None,\n):\n    \"\"\"\n    For all DataFrames in `dfs`, extrapolate to the interface defined by `bounds`.\n    \"\"\"\n    # Return results grouped by the interface and not the DataFrame\n    if group_by_interface:\n        return interface_vals_by_interface(\n            dfs,\n            bounds,\n            x_prop,\n            fit_range=fit_range,\n            y_props=y_props,\n            n=n,\n            return_fits=return_fits,\n            error_suffix=error_suffix,\n        )\n\n    results = []\n    fits = []\n    for i, (bound, df) in enumerate(zip(bounds, dfs)):\n        df_results = []\n        df_fits = []\n        for b in bound:\n            if b is not None:\n                interface_df, fit = extrapolate_to_interface(\n                    df,\n                    x_intersect=b,\n                    x_prop=x_prop,\n                    fit_range=fit_range,\n                    return_coeffs=False,\n                    return_fits=True,\n                    y_props=y_props,\n                    n=n,\n                    error_suffix=error_suffix,\n                )\n                df_results.append(interface_df)\n                if return_fits:\n                    df_fits.append(fit)\n        results.append(pd.concat(df_results))\n        if return_fits:\n            fits.append(df_fits)\n    if return_fits:\n        return results, fits\n    return results\n</code></pre>"},{"location":"reference/interface/#thermotar.intersects.masks_from_intersects","title":"<code>masks_from_intersects(dfs, intersects, x_coord, padding=None)</code>","text":"<p>From a list of intersects, create masks for the data</p> <p>Creates selections for each DataFrame that are either side of the intersect. Does the same work as bounds_from_intersects first, but then creates masks for each DataFrame.</p> <p>dfs: List[pd.DataFrame] = List of DataFrames to find intersects of. Must be in the order they appear spatially intersects: List[float] = List of intersects to create masks from x_coord: str = Name of x coordinate to find intersect in padding: float = Amount to pad the masks by. Default is 0.0</p> Source code in <code>thermotar/intersects.py</code> <pre><code>def masks_from_intersects(\n    dfs: List[pd.DataFrame],\n    intersects: List[float],\n    x_coord: str,\n    padding: float = None,\n) -&gt; List[pd.DataFrame]:\n    \"\"\"\n    From a list of intersects, create masks for the data\n\n    Creates selections for each DataFrame that are either side of the intersect.\n    Does the same work as bounds_from_intersects first, but then creates masks\n    for each DataFrame.\n\n    dfs: List[pd.DataFrame] = List of DataFrames to find intersects of. Must be in the order they appear spatially\n    intersects: List[float] = List of intersects to create masks from\n    x_coord: str = Name of x coordinate to find intersect in\n    padding: float = Amount to pad the masks by. Default is 0.0\n\n    \"\"\"\n    N = len(dfs)\n    if padding is None:\n        padding = 0.0\n    if N != len(intersects) + 1:\n        raise ValueError(\"Must always be N-1 intersects!\")\n    masks = []\n\n    for i, df in enumerate(dfs):\n        if i == 0:\n            masks.append(df[x_coord] &gt; intersects[0] - padding)\n        elif i == N - 1:\n            masks.append(df[x_coord] &lt; intersects[i - 1] + padding)\n        else:\n            masks.append(\n                (df[x_coord] &lt; intersects[i - 1] + padding)\n                | (df[x_coord] &gt; intersects[i] - padding)\n            )\n\n    if N != len(masks):\n        raise ValueError(\"There wasn't one mask per DataFrame!\")\n\n    return masks\n</code></pre>"},{"location":"reference/interface/#thermotar.intersects.multi_intersect","title":"<code>multi_intersect(dfs, x_prop, y_prop, intersect_method='linear', **intersect_kwargs)</code>","text":"<p>Insert a list of N DataFrames and find the (N-1) intersects of neighbouring data frames in the lists.</p> <p>dfs: List[pd.DataFrame] = List of DataFrames to find intersects of. Must be in the order they appear spatially</p> <p>x_prop: str = Name of x coordinate to find intersect in y_prop : str = Name of column to use to find intersections with.</p> <p>intersect_method:     'interp' : interpolates the data sets, by default linearly     'linear' : interpolates the data points linearly     'cubic' : interpolates the data points cubically to find intersects.     TODO: 'spline'</p> Source code in <code>thermotar/intersects.py</code> <pre><code>def multi_intersect(\n    dfs: List[pd.DataFrame],\n    x_prop,\n    y_prop,\n    intersect_method=\"linear\",\n    **intersect_kwargs,\n) -&gt; List[float]:\n    \"\"\"\n    Insert a list of N DataFrames and find the (N-1) intersects of neighbouring data frames in the lists.\n\n    dfs: List[pd.DataFrame] = List of DataFrames to find intersects of. Must be in the order they appear spatially\n\n    x_prop: str = Name of x coordinate to find intersect in\n    y_prop : str = Name of column to use to find intersections with.\n\n    intersect_method:\n        'interp' : interpolates the data sets, by default linearly\n        'linear' : interpolates the data points linearly\n        'cubic' : interpolates the data points cubically to find intersects.\n        TODO: 'spline'\n    \"\"\"\n    if intersect_method is None:\n        intersect_finder = PairIntersectFinders.interp\n    else:\n        try:\n            intersect_finder = pair_intesect_methods[intersect_method]\n        except KeyError:\n            raise NotImplementedError(f\"Invalid Intersect Method! {intersect_method}\")\n    intersects = []\n    # iterate over all pairs of DataFrames and fund the intersections\n    for i, (df_a, df_b) in enumerate(zip(dfs, dfs[1:])):\n        intersects.append(\n            intersect_finder(\n                df_a, df_b, y_prop=y_prop, x_prop=x_prop, **intersect_kwargs\n            )\n        )\n\n    return intersects\n</code></pre>"},{"location":"reference/thermo/","title":"The Thermo Class","text":"<p>The thermo class is used to load lammps log files, and to a lesser extent, gromacs <code>.xvg</code> files</p> <p>The primary way to create these is using <code>thermotar.create_thermos</code>, which takes in a path for the <code>LAMMPS</code> logfile and  returns either one or multiple thermo objects, depending on whether <code>join</code> and <code>last</code> are set or not.</p> <p>Defines a thermo class Thermo Data is extracted from log files</p>"},{"location":"reference/thermo/#thermotar.thermo.Thermo","title":"<code>Thermo</code>","text":"<p>Class for loading and operating on LAMMPS thermodynamic output.</p> Source code in <code>thermotar/thermo.py</code> <pre><code>class Thermo:\n    \"\"\"Class for loading and operating on LAMMPS thermodynamic output.\"\"\"\n\n    def __init__(\n        self,\n        thermo_df: pd.DataFrame,\n        *,\n        cleanup: bool = True,\n        properties: Optional[Dict[str, Any]] = None,\n    ):\n        \"\"\"\n        Construct a Thermo instance from a pandas DataFrame.\n\n        Parameters\n        ----------\n        thermo_df :\n            Pandas DataFrame containing thermodynamic information.\n        cleanup :\n            Option to remove c_ etc. prefixes from column names.\n        properties :\n            dict of properties parsed from the log file.\n            Used in create thermos or the get_props class method.\n        \"\"\"\n        self.data: pd.DataFrame = thermo_df\n\n        # clean up dataframe\n\n        if cleanup:\n            # apply strip_pref function to remove 'c_/f_/v_' prefixes to all columns\n            self.data.rename(columns=lmp_utils.strip_pref, inplace=True)\n            # replace '/' and '[]' as well as other python unfriendly characters,\n            # so these columns can be accessed with attributes\n            self.data.rename(columns=lmp_utils.drop_python_bad, inplace=True)\n\n        self.properties_dict = properties\n\n        if self.properties_dict is not None:\n            # set up properties\n            if len(self.properties_dict) &gt; 0:\n                ### TODO: set up setters and getters to the properties dict instead\n                try:\n                    self.time_step = self.properties_dict[\"time_step\"]\n                    self.box = self.properties_dict[\"box\"]\n                    # called box_Lx rather than Lx incase\n                    # it is reported via thermo output\n                    self.box_Lx = self.box[3] - self.box[0]\n                    self.box_Ly = self.box[4] - self.box[1]\n                    self.box_Lz = self.box[5] - self.box[2]\n                    self.lattice_initial = self.properties_dict[\"lattice_initial\"]\n                except KeyError:\n                    pass\n\n        # for col in self.data.columns:\n        #     setattr(self, col ,getattr(self.data, col))\n        # sets setters and getters for each column of the df as attributes of the CLASS\n        # Has to be class, not the object itself\n        df_utils.raise_columns(self)\n\n    def heat_flux(\n        self,\n        thermostat_C: str = \"thermostatC\",\n        thermostat_H: str = \"thermostatH\",\n        area: Optional[float] = None,\n        style: str = \"linear\",\n        axis: str = \"z\",\n        method: str = \"linear_fit\",\n        direction: int = 1,\n        real_2_si: bool = True,\n        tstep: Optional[float] = None,\n    ) -&gt; float:\n        \"\"\"Calculate the heatflux from the accumulated energy output.\n\n        The heatflux is calculated by linearly fitting to the `thermostat_C` and\n        `thermostat_H` columns. This assumes a steady state has been reached and the\n        heat flux is constant.\n\n        Parameters\n        ----------\n        thermostat_C  : str\n            Column name of the cold thermostat energy removal\n        thermostat_H : str\n            Column name of the hot thermostat compute\n        area : None | float | array\n            If None, work out cross sectional area from properties, if found.\n            If a float, assumes constant area along the axis,\n            If an array, take values. If style is radial, and a float,\n            this is taken to be the radius of the device\n            Default - None\n        style - str\n            Can be linear or radial atm - the geometry of the system,\n            default: linear\n        axis - str\n            Name of axis along which heat flux is applied\n            default 'z'\n\n        direction : int\n            hot to cold = 1,  cold to hot = -1 - matches\n            the sign of the thermal gradient\n\n        \"\"\"\n        # for spheriical, area needs to be a radius or an array\n        # of points for the area as a function of r\n\n        if style != \"linear\":\n            raise ValueError('Currently Only `style=\"linear\"` is supported.')\n\n        if area is None:\n            # find the area if it has been located in the thermo file metadata\n\n            if axis == \"x\":\n                area = self.box_Ly * self.box_Lz\n            elif axis == \"y\":\n                area = self.box_Lx * self.box_Lz\n            elif axis == \"z\":\n                area = self.box_Lx * self.box_Ly\n            else:\n                raise ValueError(\"axis must be x, y, or z\")\n\n        if tstep is None:\n            try:\n                tstep = self.step\n            except AttributeError:\n                raise AttributeError(\"Timestep has not been loaded from log file\")\n        try:\n            time = self.time\n        except AttributeError:\n            time = self.Step * tstep\n\n        if method == \"linear_fit\":\n            fit_H = np.polyfit(\n                time, direction * self.data[thermostat_H], 1\n            )  # so heat flows from hot to cold\n            fit_C = np.polyfit(\n                time, -1 * direction * self.data[thermostat_C], 1\n            )  # -1 * thermostat Cso heat flows from hot to cold\n\n            # average the hot and cold thermostats # second divide by 2 is accounting\n            # for the fact there are 2 fluxes in the box\n            e_flow = (fit_H[0] + fit_C[0]) / 2 / 2\n\n        if real_2_si:\n            kcal_per_mol = 4.184e3 / 6.02214076e23  # J # 1 kcal\n            # factor of 1e15 below converts to per s rather than per fs\n            # multiplication by 1e20 makes per m2 rather than per ang strom sq\n            return e_flow * kcal_per_mol * 1e15 / area / (1e-20)\n\n        return e_flow / area\n\n    @classmethod\n    def create_thermos(\n        cls, logfile, join=True, get_properties=True, last=True\n    ) -&gt; Union[List[\"Thermo\"], \"Thermo\"]:\n        \"\"\"Read the output of a lammps simulation from a logfile.\n\n        Parameters\n        ----------\n        join : bool\n            Decide whether to concatenate the thermo output of different run commands\n            into one df or not\n            If False a list of thermo objects is returned\n            default: True\n        last : bool\n            Just get the last set of data, usually production steps.\n            `last` overrides `join`.\n            default: True\n\n        \"\"\"\n        # make load thermos as  IO objects\n        strings_ios = Thermo.parse_thermo(logfile, f=StringIO)\n        # load io strings as dataframes and return as thermo object\n\n        if get_properties:\n            properties = Thermo.get_properties(logfile)\n        else:\n            properties = None\n\n        if last:\n            return Thermo(\n                pd.read_csv(strings_ios[-1], sep=r\"\\s+\"), properties=properties\n            )\n        if not join:\n            return [\n                Thermo(pd.read_csv(csv, sep=r\"\\s+\"), properties=properties)\n                for csv in strings_ios\n            ]\n\n        else:\n            joined_df = pd.concat(\n                [pd.read_csv(csv, sep=r\"\\s+\") for csv in strings_ios]\n            ).reset_index()\n\n            return Thermo(joined_df, properties=properties)\n\n    @classmethod\n    def parse_thermo(cls, logfile: Union[str, os.PathLike], f=None) -&gt; List[str]:\n        \"\"\"Parse thermo data into strings.\n\n        This is primarily meant to e aan internal method.\n        Reads the given LAMMPS log file and outputs a list of strings that\n        contain each thermo time series.\n\n        An optional argument f is applied to list of strings before returning,\n        for code reusability\n\n        Parameters\n        ----------\n        logfile:\n            Filename or path to read the logfile from.\n        f:\n            A function that is applied to all found thermos.\n\n        \"\"\"\n        # TODO: Make more efficient\n        # todo perhaps change to output thermo objects???\n        # todo add automatic skipping of lines with the wrong number of rows\n        # todo if no 'Per MPI rank' found treat as if the file is a tab separated file\n\n        thermo_datas = []\n\n        with open(logfile, \"r\") as stream:\n            current_thermo = \"\"\n            thermo_start = False\n            warnings_found = False\n\n            # Parse file to find thermo data, returns as list of strings\n            for line in stream:\n                if line.startswith(\n                    r\"Per MPI rank\"\n                ):  #### todo use regular expression instead???\n                    current_thermo = \"\"\n                    thermo_start = True\n                elif (\n                    line.startswith(r\"Loop time of\")\n                    or line.startswith(r\"ERROR\")\n                    or line.startswith(\"colvars: Saving collective\")\n                ):  ### this used to create a bug when the thing errors out\n                    thermo_datas.append(current_thermo)\n                    thermo_start = False\n                elif line.startswith(\"WARNING\"):\n                    # If the line is a warning, skip to the next line\n                    warnings_found = True\n                    continue\n                elif thermo_start:\n                    current_thermo += line\n\n        if thermo_start:\n            # if the thermo series is incomplete appends it anyway\n            thermo_datas.append(current_thermo)\n\n        # TODO Gather warnings and emit them\n        if warnings_found:\n            warnings.warn(\"Warnings found when reading File\")\n\n        # if len(thermo_datas) ==0:\n        #     try:\n        #         #load as file\n\n        if f:\n            return [\n                f(i) for i in thermo_datas\n            ]  # applies function to string, default is to do nothing\n        else:\n            return thermo_datas\n\n    @staticmethod\n    def _split_thermo(\n        logfile, path=\"./split_thermos/\", file_name_format=\"thermo_{}.csv\", **kwargs\n    ):\n        # todo make class method???\n        thermo_lists = Thermo.parse_thermo(logfile)\n\n        try:\n            os.mkdir(path)\n        except FileExistsError:\n            pass\n\n        for i, thermo in enumerate(thermo_lists):\n            out_file = open(path + file_name_format.format(i), \"w\")\n            out_file.write(thermo)\n            out_file.close()\n\n        return thermo_lists\n\n    @staticmethod\n    def from_csv(csv_file: Path, **kwargs) -&gt; \"Thermo\":\n        \"\"\"Create a Thermo object from a csv file.\n\n        Parameters\n        ----------\n        csv_file:\n            path to csv file\n        kwargs: keyword arguments to pass to `pandas.read_csv`\n        \"\"\"\n        return Thermo(pd.read_csv(csv_file, **kwargs))\n\n    @staticmethod\n    def get_properties(logfile: Union[str, os.PathLike]):\n        \"\"\"Extract non timeseries 'properties' from the logfile.\n\n        Currently tries to extract the timestep, lattice size and box size.\n\n        Some of these can only be read if the logfile was written from stdout\n        of the lammps simulations rather than from the -log flag.\n\n        Parameters\n        ----------\n        logfile:\n            The name of the lammps logfile to read.\n\n\n        \"\"\"\n        properties_dict = parse_logs.get_lmp_properties(logfile)\n\n        return properties_dict\n\n    def plot_property(\n        self, therm_property: str, x_property: Optional[str] = None, **kwargs\n    ):\n        \"\"\"\n        Plot the provieded properties against eachother.\n\n        By default `therm_property` is plotted against the Step or Time, in that order.\n\n        Parameters\n        ----------\n        therm_property:\n            Which property is plotted on the y-axis\n        x_property:\n            Plot this on the x-axis. If not provided plots against the Step or Time.\n\n        \"\"\"\n        # todo allow plotting many properties at once\n\n        the_data = self.data\n\n        if therm_property not in the_data.keys():\n            raise KeyError(f\"Property {therm_property} not found\")\n\n        if x_property is None:\n            if \"Step\" in self.data.keys():\n                x_property = \"Step\"\n            elif \"Time\" in self.data.keys():\n                x_property = \"Time\"\n            else:\n                x_property = None\n\n        # print('got this far!!!')\n\n        return self.data.plot(\n            x=x_property, y=therm_property, ls=\" \", marker=\"x\", **kwargs\n        )\n\n    def reverse_cum_average(self, property):\n        \"\"\"Calculate the cumulative average in larger and larger chunks.\"\"\"\n        prop = self.data[property]\n\n        cum_ave = np.array([np.mean(prop[i:]) for i, point in enumerate(prop)])\n\n        return pd.Series(cum_ave)\n\n    def compare_dist(self, property, bins=100, n_blocks=5, **kwargs):\n        \"\"\"\n        Plot the data as a histogram as well as the estimated probability density function.\n        Also plot the gaussian that has the estimated mean and standard deviation.\n\n        [!note]\n            These do not correspond to good estimates. Sub averages should be plotted instead.\n            The standard deviation of the gaussian is not the standard error.\n\n        Parameters:\n            property: name of the property to plot\n            bins: number of bins to use for the histogram\n            n_blocks: number of blocks to use for the error estimate\n            kwargs: keyword arguments to pass to the plotting functions\n        \"\"\"\n        # TODO: Use it or lose it:\n        # from scipy import stats\n\n        # Estimate error of the property\n        ave_err = self.estimate_error(n_blocks=n_blocks)\n        ave = float(ave_err[\"ave\"].loc[property])\n        # TODO: Use it or lose it:\n        # err = float(ave_err[\"sem\"].loc[property])\n\n        _, ax = plt.subplots(1)\n\n        self.data[property].plot.density(**kwargs, label=\"PDF\", ax=ax)\n        self.data[property].plot.hist(\n            **kwargs, density=True, bins=bins, label=\"Histogram\", ax=ax\n        )\n        ax.axvline(ave, color=\"k\", linestyle=\"dashed\", linewidth=1, label=\"Mean\")\n        # x = np.linspace(ave - 3 * err , ave + 3 * err,500)\n        # ax.plot(x, stats.norm.pdf(x,ave,err), label=\"Gaussian\")\n\n    def compare_dist_samples(self, property, n_samples=100, **kwargs):\n        \"\"\"\n        Plot the data as a histogram as well as the estimated probability density function.\n        Also plot the gaussian that has the estimated mean and standard deviation.\n\n        Parameters:\n            property: name of the property to plot\n            n_samples: number of sub-averages used.\n            kwargs: keyword arguments to pass to the plotting functions\n        \"\"\"\n        from scipy import stats\n\n        df = self.block_aves(n_blocks=n_samples)[property]\n\n        # Estimate error of the property\n        ave = df.mean()\n        err = df.std()\n\n        _, ax = plt.subplots(1)\n\n        df.plot.density(**kwargs, label=\"PDF\", ax=ax)\n        df.plot.hist(**kwargs, density=True, bins=n_samples, label=\"Histogram\", ax=ax)\n        ax.axvline(ave, color=\"k\", linestyle=\"dashed\", linewidth=1, label=\"Mean\")\n        x = np.linspace(ave - 3 * err, ave + 3 * err, 500)\n        ax.plot(x, stats.norm.pdf(x, ave, err), label=\"Gaussian\")\n\n    def block_aves(\n        self,\n        group_col=\"Step\",\n        n_blocks=5,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Divide the simulation into `n_blocks` and take the average of each block.\n\n        Used for the calculation of error estimates from a single simulation trajectory.\n\n        Parameters\n        ----------\n        group_col :\n            Which column to use for splitting the time series into bins.\n        n_blocks :\n            How many bins to use.\n\n        Returns\n        -------\n        Returns a dataframe with the block number as the index and the properties as\n        the columns.\n\n        \"\"\"\n        bw = df_utils.n_blocks2bw(self.data[group_col], n_blocks)\n\n        return df_utils.rebin(self.data, binning_coord=group_col, bw=bw)\n\n    def estimate_error(\n        self, group_col=\"Step\", n_blocks=5, error_calc=\"sem\", error_label=\"err\"\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Block averaging estimates for the error of the mean and error in the data.\n\n        Parameters\n        ----------\n        group_col:\n            Column to group the data by. Typically \"Step\" or \"Time\"\n        n_blocks:\n            Number of blocks to divide the thermo data into.\n        error_calc:\n            Method of estimating the error. Either \"sem\" or \"std\". Default \"sem\"\n        error_label:\n            Suffix appended to error columns, joined by a \"_\". Default: \"err\"\n\n        Returns\n        -------\n        A DataFrame with a multi index with an average and error for each property.\n\n        Changes in version 0.0.2:\n            Error columns now have \"_err\" as suffix by default instead of the value of\n            `error_calc`. It can be set with `error_label` to overcome this.\n        \"\"\"\n        aves = self.block_aves(group_col=group_col, n_blocks=n_blocks)\n\n        ave_df = aves.mean()\n\n        if error_calc == \"sem\":\n            error_df = aves.sem()\n        elif error_calc == \"std\":\n            error_df = aves.std()\n        else:\n            raise ValueError(\"Only sem and std are valid error calculation types.\")\n\n        # error_df = error_method()\n\n        # TODO Change sem/std to err?\n        return pd.DataFrame({\"ave\": ave_df, f\"{error_label}\": error_df})\n\n    def estimate_drift(self, time_coord: str = \"Step\") -&gt; pd.DataFrame:\n        \"\"\"Estimate the percentage drift in the thermodynamic properties, by performing linear fits.\n\n        The percentage drift is relative to the starting fitted value.\n        If the fitting for the drift estimate fails, the parameters are set to np.nan\n        \"\"\"\n        df = self.data\n\n        cols = set(df.columns)\n        # Only non-time properties\n        cols = cols.difference({time_coord})\n\n        def drift_col(x: pd.Series, col: pd.Series) -&gt; Dict[str, float]:\n            try:\n                fit = np.polyfit(x=x, y=col, deg=1)\n                y_start = np.polyval(fit, x.iloc[0])\n                y_end = np.polyval(fit, x.iloc[-1])\n                drift = y_start - y_end\n\n                return {\"drift\": drift, \"frac_drift\": drift / y_start}\n            except np.linalg.LinAlgError:\n                return {\"drift\": np.nan, \"frac_drift\": np.nan}\n\n        drifts = pd.DataFrame.from_dict(\n            {col: drift_col(df[time_coord], df[col]) for col in cols},\n        )\n\n        # TODO: fit to all the columns and calculate the high and low values and the percentage drift.\n        return drifts\n\n    def stats(self, n_blocks: Optional[int] = None) -&gt; pd.DataFrame:\n        \"\"\"Compute summary statisitics of the simulation. Optionally compute block into bins first.\"\"\"\n        if n_blocks is not None:\n            df = self.block_aves(n_blocks=n_blocks)\n        else:\n            df = self.data\n\n        return df.describe()\n\n    # Dunder methods.\n    def __repr__(self) -&gt; str:\n        \"\"\"Pretty print.\"\"\"\n        return f\"Thermo({self.data})\"\n\n    def __getitem__(self, key: str):\n        \"\"\"Access the underlying dataframe columns.\"\"\"\n        return self.data[key]\n</code></pre>"},{"location":"reference/thermo/#thermotar.thermo.Thermo.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Access the underlying dataframe columns.</p> Source code in <code>thermotar/thermo.py</code> <pre><code>def __getitem__(self, key: str):\n    \"\"\"Access the underlying dataframe columns.\"\"\"\n    return self.data[key]\n</code></pre>"},{"location":"reference/thermo/#thermotar.thermo.Thermo.__init__","title":"<code>__init__(thermo_df, *, cleanup=True, properties=None)</code>","text":"<p>Construct a Thermo instance from a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>thermo_df</code> <code>DataFrame</code> <p>Pandas DataFrame containing thermodynamic information.</p> required <code>cleanup</code> <code>bool</code> <p>Option to remove c_ etc. prefixes from column names.</p> <code>True</code> <code>properties</code> <code>Optional[Dict[str, Any]]</code> <p>dict of properties parsed from the log file. Used in create thermos or the get_props class method.</p> <code>None</code> Source code in <code>thermotar/thermo.py</code> <pre><code>def __init__(\n    self,\n    thermo_df: pd.DataFrame,\n    *,\n    cleanup: bool = True,\n    properties: Optional[Dict[str, Any]] = None,\n):\n    \"\"\"\n    Construct a Thermo instance from a pandas DataFrame.\n\n    Parameters\n    ----------\n    thermo_df :\n        Pandas DataFrame containing thermodynamic information.\n    cleanup :\n        Option to remove c_ etc. prefixes from column names.\n    properties :\n        dict of properties parsed from the log file.\n        Used in create thermos or the get_props class method.\n    \"\"\"\n    self.data: pd.DataFrame = thermo_df\n\n    # clean up dataframe\n\n    if cleanup:\n        # apply strip_pref function to remove 'c_/f_/v_' prefixes to all columns\n        self.data.rename(columns=lmp_utils.strip_pref, inplace=True)\n        # replace '/' and '[]' as well as other python unfriendly characters,\n        # so these columns can be accessed with attributes\n        self.data.rename(columns=lmp_utils.drop_python_bad, inplace=True)\n\n    self.properties_dict = properties\n\n    if self.properties_dict is not None:\n        # set up properties\n        if len(self.properties_dict) &gt; 0:\n            ### TODO: set up setters and getters to the properties dict instead\n            try:\n                self.time_step = self.properties_dict[\"time_step\"]\n                self.box = self.properties_dict[\"box\"]\n                # called box_Lx rather than Lx incase\n                # it is reported via thermo output\n                self.box_Lx = self.box[3] - self.box[0]\n                self.box_Ly = self.box[4] - self.box[1]\n                self.box_Lz = self.box[5] - self.box[2]\n                self.lattice_initial = self.properties_dict[\"lattice_initial\"]\n            except KeyError:\n                pass\n\n    # for col in self.data.columns:\n    #     setattr(self, col ,getattr(self.data, col))\n    # sets setters and getters for each column of the df as attributes of the CLASS\n    # Has to be class, not the object itself\n    df_utils.raise_columns(self)\n</code></pre>"},{"location":"reference/thermo/#thermotar.thermo.Thermo.__repr__","title":"<code>__repr__()</code>","text":"<p>Pretty print.</p> Source code in <code>thermotar/thermo.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Pretty print.\"\"\"\n    return f\"Thermo({self.data})\"\n</code></pre>"},{"location":"reference/thermo/#thermotar.thermo.Thermo.block_aves","title":"<code>block_aves(group_col='Step', n_blocks=5)</code>","text":"<p>Divide the simulation into <code>n_blocks</code> and take the average of each block.</p> <p>Used for the calculation of error estimates from a single simulation trajectory.</p> <p>Parameters:</p> Name Type Description Default <code>group_col</code> <p>Which column to use for splitting the time series into bins.</p> <code>'Step'</code> <code>n_blocks</code> <p>How many bins to use.</p> <code>5</code> <p>Returns:</p> Type Description <code>Returns a dataframe with the block number as the index and the properties as</code> <code>the columns.</code> Source code in <code>thermotar/thermo.py</code> <pre><code>def block_aves(\n    self,\n    group_col=\"Step\",\n    n_blocks=5,\n) -&gt; pd.DataFrame:\n    \"\"\"Divide the simulation into `n_blocks` and take the average of each block.\n\n    Used for the calculation of error estimates from a single simulation trajectory.\n\n    Parameters\n    ----------\n    group_col :\n        Which column to use for splitting the time series into bins.\n    n_blocks :\n        How many bins to use.\n\n    Returns\n    -------\n    Returns a dataframe with the block number as the index and the properties as\n    the columns.\n\n    \"\"\"\n    bw = df_utils.n_blocks2bw(self.data[group_col], n_blocks)\n\n    return df_utils.rebin(self.data, binning_coord=group_col, bw=bw)\n</code></pre>"},{"location":"reference/thermo/#thermotar.thermo.Thermo.compare_dist","title":"<code>compare_dist(property, bins=100, n_blocks=5, **kwargs)</code>","text":"<p>Plot the data as a histogram as well as the estimated probability density function. Also plot the gaussian that has the estimated mean and standard deviation.</p> <p>[!note]     These do not correspond to good estimates. Sub averages should be plotted instead.     The standard deviation of the gaussian is not the standard error.</p> <p>Parameters:     property: name of the property to plot     bins: number of bins to use for the histogram     n_blocks: number of blocks to use for the error estimate     kwargs: keyword arguments to pass to the plotting functions</p> Source code in <code>thermotar/thermo.py</code> <pre><code>def compare_dist(self, property, bins=100, n_blocks=5, **kwargs):\n    \"\"\"\n    Plot the data as a histogram as well as the estimated probability density function.\n    Also plot the gaussian that has the estimated mean and standard deviation.\n\n    [!note]\n        These do not correspond to good estimates. Sub averages should be plotted instead.\n        The standard deviation of the gaussian is not the standard error.\n\n    Parameters:\n        property: name of the property to plot\n        bins: number of bins to use for the histogram\n        n_blocks: number of blocks to use for the error estimate\n        kwargs: keyword arguments to pass to the plotting functions\n    \"\"\"\n    # TODO: Use it or lose it:\n    # from scipy import stats\n\n    # Estimate error of the property\n    ave_err = self.estimate_error(n_blocks=n_blocks)\n    ave = float(ave_err[\"ave\"].loc[property])\n    # TODO: Use it or lose it:\n    # err = float(ave_err[\"sem\"].loc[property])\n\n    _, ax = plt.subplots(1)\n\n    self.data[property].plot.density(**kwargs, label=\"PDF\", ax=ax)\n    self.data[property].plot.hist(\n        **kwargs, density=True, bins=bins, label=\"Histogram\", ax=ax\n    )\n    ax.axvline(ave, color=\"k\", linestyle=\"dashed\", linewidth=1, label=\"Mean\")\n</code></pre>"},{"location":"reference/thermo/#thermotar.thermo.Thermo.compare_dist_samples","title":"<code>compare_dist_samples(property, n_samples=100, **kwargs)</code>","text":"<p>Plot the data as a histogram as well as the estimated probability density function. Also plot the gaussian that has the estimated mean and standard deviation.</p> <p>Parameters:     property: name of the property to plot     n_samples: number of sub-averages used.     kwargs: keyword arguments to pass to the plotting functions</p> Source code in <code>thermotar/thermo.py</code> <pre><code>def compare_dist_samples(self, property, n_samples=100, **kwargs):\n    \"\"\"\n    Plot the data as a histogram as well as the estimated probability density function.\n    Also plot the gaussian that has the estimated mean and standard deviation.\n\n    Parameters:\n        property: name of the property to plot\n        n_samples: number of sub-averages used.\n        kwargs: keyword arguments to pass to the plotting functions\n    \"\"\"\n    from scipy import stats\n\n    df = self.block_aves(n_blocks=n_samples)[property]\n\n    # Estimate error of the property\n    ave = df.mean()\n    err = df.std()\n\n    _, ax = plt.subplots(1)\n\n    df.plot.density(**kwargs, label=\"PDF\", ax=ax)\n    df.plot.hist(**kwargs, density=True, bins=n_samples, label=\"Histogram\", ax=ax)\n    ax.axvline(ave, color=\"k\", linestyle=\"dashed\", linewidth=1, label=\"Mean\")\n    x = np.linspace(ave - 3 * err, ave + 3 * err, 500)\n    ax.plot(x, stats.norm.pdf(x, ave, err), label=\"Gaussian\")\n</code></pre>"},{"location":"reference/thermo/#thermotar.thermo.Thermo.create_thermos","title":"<code>create_thermos(logfile, join=True, get_properties=True, last=True)</code>  <code>classmethod</code>","text":"<p>Read the output of a lammps simulation from a logfile.</p> <p>Parameters:</p> Name Type Description Default <code>join</code> <code>bool</code> <p>Decide whether to concatenate the thermo output of different run commands into one df or not If False a list of thermo objects is returned default: True</p> <code>True</code> <code>last</code> <code>bool</code> <p>Just get the last set of data, usually production steps. <code>last</code> overrides <code>join</code>. default: True</p> <code>True</code> Source code in <code>thermotar/thermo.py</code> <pre><code>@classmethod\ndef create_thermos(\n    cls, logfile, join=True, get_properties=True, last=True\n) -&gt; Union[List[\"Thermo\"], \"Thermo\"]:\n    \"\"\"Read the output of a lammps simulation from a logfile.\n\n    Parameters\n    ----------\n    join : bool\n        Decide whether to concatenate the thermo output of different run commands\n        into one df or not\n        If False a list of thermo objects is returned\n        default: True\n    last : bool\n        Just get the last set of data, usually production steps.\n        `last` overrides `join`.\n        default: True\n\n    \"\"\"\n    # make load thermos as  IO objects\n    strings_ios = Thermo.parse_thermo(logfile, f=StringIO)\n    # load io strings as dataframes and return as thermo object\n\n    if get_properties:\n        properties = Thermo.get_properties(logfile)\n    else:\n        properties = None\n\n    if last:\n        return Thermo(\n            pd.read_csv(strings_ios[-1], sep=r\"\\s+\"), properties=properties\n        )\n    if not join:\n        return [\n            Thermo(pd.read_csv(csv, sep=r\"\\s+\"), properties=properties)\n            for csv in strings_ios\n        ]\n\n    else:\n        joined_df = pd.concat(\n            [pd.read_csv(csv, sep=r\"\\s+\") for csv in strings_ios]\n        ).reset_index()\n\n        return Thermo(joined_df, properties=properties)\n</code></pre>"},{"location":"reference/thermo/#thermotar.thermo.Thermo.estimate_drift","title":"<code>estimate_drift(time_coord='Step')</code>","text":"<p>Estimate the percentage drift in the thermodynamic properties, by performing linear fits.</p> <p>The percentage drift is relative to the starting fitted value. If the fitting for the drift estimate fails, the parameters are set to np.nan</p> Source code in <code>thermotar/thermo.py</code> <pre><code>def estimate_drift(self, time_coord: str = \"Step\") -&gt; pd.DataFrame:\n    \"\"\"Estimate the percentage drift in the thermodynamic properties, by performing linear fits.\n\n    The percentage drift is relative to the starting fitted value.\n    If the fitting for the drift estimate fails, the parameters are set to np.nan\n    \"\"\"\n    df = self.data\n\n    cols = set(df.columns)\n    # Only non-time properties\n    cols = cols.difference({time_coord})\n\n    def drift_col(x: pd.Series, col: pd.Series) -&gt; Dict[str, float]:\n        try:\n            fit = np.polyfit(x=x, y=col, deg=1)\n            y_start = np.polyval(fit, x.iloc[0])\n            y_end = np.polyval(fit, x.iloc[-1])\n            drift = y_start - y_end\n\n            return {\"drift\": drift, \"frac_drift\": drift / y_start}\n        except np.linalg.LinAlgError:\n            return {\"drift\": np.nan, \"frac_drift\": np.nan}\n\n    drifts = pd.DataFrame.from_dict(\n        {col: drift_col(df[time_coord], df[col]) for col in cols},\n    )\n\n    # TODO: fit to all the columns and calculate the high and low values and the percentage drift.\n    return drifts\n</code></pre>"},{"location":"reference/thermo/#thermotar.thermo.Thermo.estimate_error","title":"<code>estimate_error(group_col='Step', n_blocks=5, error_calc='sem', error_label='err')</code>","text":"<p>Block averaging estimates for the error of the mean and error in the data.</p> <p>Parameters:</p> Name Type Description Default <code>group_col</code> <p>Column to group the data by. Typically \"Step\" or \"Time\"</p> <code>'Step'</code> <code>n_blocks</code> <p>Number of blocks to divide the thermo data into.</p> <code>5</code> <code>error_calc</code> <p>Method of estimating the error. Either \"sem\" or \"std\". Default \"sem\"</p> <code>'sem'</code> <code>error_label</code> <p>Suffix appended to error columns, joined by a \"_\". Default: \"err\"</p> <code>'err'</code> <p>Returns:</p> Type Description <code>A DataFrame with a multi index with an average and error for each property.</code> <code>Changes in version 0.0.2:</code> <p>Error columns now have \"_err\" as suffix by default instead of the value of <code>error_calc</code>. It can be set with <code>error_label</code> to overcome this.</p> Source code in <code>thermotar/thermo.py</code> <pre><code>def estimate_error(\n    self, group_col=\"Step\", n_blocks=5, error_calc=\"sem\", error_label=\"err\"\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Block averaging estimates for the error of the mean and error in the data.\n\n    Parameters\n    ----------\n    group_col:\n        Column to group the data by. Typically \"Step\" or \"Time\"\n    n_blocks:\n        Number of blocks to divide the thermo data into.\n    error_calc:\n        Method of estimating the error. Either \"sem\" or \"std\". Default \"sem\"\n    error_label:\n        Suffix appended to error columns, joined by a \"_\". Default: \"err\"\n\n    Returns\n    -------\n    A DataFrame with a multi index with an average and error for each property.\n\n    Changes in version 0.0.2:\n        Error columns now have \"_err\" as suffix by default instead of the value of\n        `error_calc`. It can be set with `error_label` to overcome this.\n    \"\"\"\n    aves = self.block_aves(group_col=group_col, n_blocks=n_blocks)\n\n    ave_df = aves.mean()\n\n    if error_calc == \"sem\":\n        error_df = aves.sem()\n    elif error_calc == \"std\":\n        error_df = aves.std()\n    else:\n        raise ValueError(\"Only sem and std are valid error calculation types.\")\n\n    # error_df = error_method()\n\n    # TODO Change sem/std to err?\n    return pd.DataFrame({\"ave\": ave_df, f\"{error_label}\": error_df})\n</code></pre>"},{"location":"reference/thermo/#thermotar.thermo.Thermo.from_csv","title":"<code>from_csv(csv_file, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a Thermo object from a csv file.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>Path</code> <p>path to csv file</p> required <code>kwargs</code> <code>{}</code> Source code in <code>thermotar/thermo.py</code> <pre><code>@staticmethod\ndef from_csv(csv_file: Path, **kwargs) -&gt; \"Thermo\":\n    \"\"\"Create a Thermo object from a csv file.\n\n    Parameters\n    ----------\n    csv_file:\n        path to csv file\n    kwargs: keyword arguments to pass to `pandas.read_csv`\n    \"\"\"\n    return Thermo(pd.read_csv(csv_file, **kwargs))\n</code></pre>"},{"location":"reference/thermo/#thermotar.thermo.Thermo.get_properties","title":"<code>get_properties(logfile)</code>  <code>staticmethod</code>","text":"<p>Extract non timeseries 'properties' from the logfile.</p> <p>Currently tries to extract the timestep, lattice size and box size.</p> <p>Some of these can only be read if the logfile was written from stdout of the lammps simulations rather than from the -log flag.</p> <p>Parameters:</p> Name Type Description Default <code>logfile</code> <code>Union[str, PathLike]</code> <p>The name of the lammps logfile to read.</p> required Source code in <code>thermotar/thermo.py</code> <pre><code>@staticmethod\ndef get_properties(logfile: Union[str, os.PathLike]):\n    \"\"\"Extract non timeseries 'properties' from the logfile.\n\n    Currently tries to extract the timestep, lattice size and box size.\n\n    Some of these can only be read if the logfile was written from stdout\n    of the lammps simulations rather than from the -log flag.\n\n    Parameters\n    ----------\n    logfile:\n        The name of the lammps logfile to read.\n\n\n    \"\"\"\n    properties_dict = parse_logs.get_lmp_properties(logfile)\n\n    return properties_dict\n</code></pre>"},{"location":"reference/thermo/#thermotar.thermo.Thermo.heat_flux","title":"<code>heat_flux(thermostat_C='thermostatC', thermostat_H='thermostatH', area=None, style='linear', axis='z', method='linear_fit', direction=1, real_2_si=True, tstep=None)</code>","text":"<p>Calculate the heatflux from the accumulated energy output.</p> <p>The heatflux is calculated by linearly fitting to the <code>thermostat_C</code> and <code>thermostat_H</code> columns. This assumes a steady state has been reached and the heat flux is constant.</p> <p>Parameters:</p> Name Type Description Default <code>thermostat_C</code> <code>str</code> <p>Column name of the cold thermostat energy removal</p> <code>'thermostatC'</code> <code>thermostat_H</code> <code>str</code> <p>Column name of the hot thermostat compute</p> <code>'thermostatH'</code> <code>area</code> <code>None | float | array</code> <p>If None, work out cross sectional area from properties, if found. If a float, assumes constant area along the axis, If an array, take values. If style is radial, and a float, this is taken to be the radius of the device Default - None</p> <code>None</code> <code>style</code> <code>str</code> <p>Can be linear or radial atm - the geometry of the system, default: linear</p> <code>'linear'</code> <code>axis</code> <code>str</code> <p>Name of axis along which heat flux is applied default 'z'</p> <code>'z'</code> <code>direction</code> <code>int</code> <p>hot to cold = 1,  cold to hot = -1 - matches the sign of the thermal gradient</p> <code>1</code> Source code in <code>thermotar/thermo.py</code> <pre><code>def heat_flux(\n    self,\n    thermostat_C: str = \"thermostatC\",\n    thermostat_H: str = \"thermostatH\",\n    area: Optional[float] = None,\n    style: str = \"linear\",\n    axis: str = \"z\",\n    method: str = \"linear_fit\",\n    direction: int = 1,\n    real_2_si: bool = True,\n    tstep: Optional[float] = None,\n) -&gt; float:\n    \"\"\"Calculate the heatflux from the accumulated energy output.\n\n    The heatflux is calculated by linearly fitting to the `thermostat_C` and\n    `thermostat_H` columns. This assumes a steady state has been reached and the\n    heat flux is constant.\n\n    Parameters\n    ----------\n    thermostat_C  : str\n        Column name of the cold thermostat energy removal\n    thermostat_H : str\n        Column name of the hot thermostat compute\n    area : None | float | array\n        If None, work out cross sectional area from properties, if found.\n        If a float, assumes constant area along the axis,\n        If an array, take values. If style is radial, and a float,\n        this is taken to be the radius of the device\n        Default - None\n    style - str\n        Can be linear or radial atm - the geometry of the system,\n        default: linear\n    axis - str\n        Name of axis along which heat flux is applied\n        default 'z'\n\n    direction : int\n        hot to cold = 1,  cold to hot = -1 - matches\n        the sign of the thermal gradient\n\n    \"\"\"\n    # for spheriical, area needs to be a radius or an array\n    # of points for the area as a function of r\n\n    if style != \"linear\":\n        raise ValueError('Currently Only `style=\"linear\"` is supported.')\n\n    if area is None:\n        # find the area if it has been located in the thermo file metadata\n\n        if axis == \"x\":\n            area = self.box_Ly * self.box_Lz\n        elif axis == \"y\":\n            area = self.box_Lx * self.box_Lz\n        elif axis == \"z\":\n            area = self.box_Lx * self.box_Ly\n        else:\n            raise ValueError(\"axis must be x, y, or z\")\n\n    if tstep is None:\n        try:\n            tstep = self.step\n        except AttributeError:\n            raise AttributeError(\"Timestep has not been loaded from log file\")\n    try:\n        time = self.time\n    except AttributeError:\n        time = self.Step * tstep\n\n    if method == \"linear_fit\":\n        fit_H = np.polyfit(\n            time, direction * self.data[thermostat_H], 1\n        )  # so heat flows from hot to cold\n        fit_C = np.polyfit(\n            time, -1 * direction * self.data[thermostat_C], 1\n        )  # -1 * thermostat Cso heat flows from hot to cold\n\n        # average the hot and cold thermostats # second divide by 2 is accounting\n        # for the fact there are 2 fluxes in the box\n        e_flow = (fit_H[0] + fit_C[0]) / 2 / 2\n\n    if real_2_si:\n        kcal_per_mol = 4.184e3 / 6.02214076e23  # J # 1 kcal\n        # factor of 1e15 below converts to per s rather than per fs\n        # multiplication by 1e20 makes per m2 rather than per ang strom sq\n        return e_flow * kcal_per_mol * 1e15 / area / (1e-20)\n\n    return e_flow / area\n</code></pre>"},{"location":"reference/thermo/#thermotar.thermo.Thermo.parse_thermo","title":"<code>parse_thermo(logfile, f=None)</code>  <code>classmethod</code>","text":"<p>Parse thermo data into strings.</p> <p>This is primarily meant to e aan internal method. Reads the given LAMMPS log file and outputs a list of strings that contain each thermo time series.</p> <p>An optional argument f is applied to list of strings before returning, for code reusability</p> <p>Parameters:</p> Name Type Description Default <code>logfile</code> <code>Union[str, PathLike]</code> <p>Filename or path to read the logfile from.</p> required <code>f</code> <p>A function that is applied to all found thermos.</p> <code>None</code> Source code in <code>thermotar/thermo.py</code> <pre><code>@classmethod\ndef parse_thermo(cls, logfile: Union[str, os.PathLike], f=None) -&gt; List[str]:\n    \"\"\"Parse thermo data into strings.\n\n    This is primarily meant to e aan internal method.\n    Reads the given LAMMPS log file and outputs a list of strings that\n    contain each thermo time series.\n\n    An optional argument f is applied to list of strings before returning,\n    for code reusability\n\n    Parameters\n    ----------\n    logfile:\n        Filename or path to read the logfile from.\n    f:\n        A function that is applied to all found thermos.\n\n    \"\"\"\n    # TODO: Make more efficient\n    # todo perhaps change to output thermo objects???\n    # todo add automatic skipping of lines with the wrong number of rows\n    # todo if no 'Per MPI rank' found treat as if the file is a tab separated file\n\n    thermo_datas = []\n\n    with open(logfile, \"r\") as stream:\n        current_thermo = \"\"\n        thermo_start = False\n        warnings_found = False\n\n        # Parse file to find thermo data, returns as list of strings\n        for line in stream:\n            if line.startswith(\n                r\"Per MPI rank\"\n            ):  #### todo use regular expression instead???\n                current_thermo = \"\"\n                thermo_start = True\n            elif (\n                line.startswith(r\"Loop time of\")\n                or line.startswith(r\"ERROR\")\n                or line.startswith(\"colvars: Saving collective\")\n            ):  ### this used to create a bug when the thing errors out\n                thermo_datas.append(current_thermo)\n                thermo_start = False\n            elif line.startswith(\"WARNING\"):\n                # If the line is a warning, skip to the next line\n                warnings_found = True\n                continue\n            elif thermo_start:\n                current_thermo += line\n\n    if thermo_start:\n        # if the thermo series is incomplete appends it anyway\n        thermo_datas.append(current_thermo)\n\n    # TODO Gather warnings and emit them\n    if warnings_found:\n        warnings.warn(\"Warnings found when reading File\")\n\n    # if len(thermo_datas) ==0:\n    #     try:\n    #         #load as file\n\n    if f:\n        return [\n            f(i) for i in thermo_datas\n        ]  # applies function to string, default is to do nothing\n    else:\n        return thermo_datas\n</code></pre>"},{"location":"reference/thermo/#thermotar.thermo.Thermo.plot_property","title":"<code>plot_property(therm_property, x_property=None, **kwargs)</code>","text":"<p>Plot the provieded properties against eachother.</p> <p>By default <code>therm_property</code> is plotted against the Step or Time, in that order.</p> <p>Parameters:</p> Name Type Description Default <code>therm_property</code> <code>str</code> <p>Which property is plotted on the y-axis</p> required <code>x_property</code> <code>Optional[str]</code> <p>Plot this on the x-axis. If not provided plots against the Step or Time.</p> <code>None</code> Source code in <code>thermotar/thermo.py</code> <pre><code>def plot_property(\n    self, therm_property: str, x_property: Optional[str] = None, **kwargs\n):\n    \"\"\"\n    Plot the provieded properties against eachother.\n\n    By default `therm_property` is plotted against the Step or Time, in that order.\n\n    Parameters\n    ----------\n    therm_property:\n        Which property is plotted on the y-axis\n    x_property:\n        Plot this on the x-axis. If not provided plots against the Step or Time.\n\n    \"\"\"\n    # todo allow plotting many properties at once\n\n    the_data = self.data\n\n    if therm_property not in the_data.keys():\n        raise KeyError(f\"Property {therm_property} not found\")\n\n    if x_property is None:\n        if \"Step\" in self.data.keys():\n            x_property = \"Step\"\n        elif \"Time\" in self.data.keys():\n            x_property = \"Time\"\n        else:\n            x_property = None\n\n    # print('got this far!!!')\n\n    return self.data.plot(\n        x=x_property, y=therm_property, ls=\" \", marker=\"x\", **kwargs\n    )\n</code></pre>"},{"location":"reference/thermo/#thermotar.thermo.Thermo.reverse_cum_average","title":"<code>reverse_cum_average(property)</code>","text":"<p>Calculate the cumulative average in larger and larger chunks.</p> Source code in <code>thermotar/thermo.py</code> <pre><code>def reverse_cum_average(self, property):\n    \"\"\"Calculate the cumulative average in larger and larger chunks.\"\"\"\n    prop = self.data[property]\n\n    cum_ave = np.array([np.mean(prop[i:]) for i, point in enumerate(prop)])\n\n    return pd.Series(cum_ave)\n</code></pre>"},{"location":"reference/thermo/#thermotar.thermo.Thermo.stats","title":"<code>stats(n_blocks=None)</code>","text":"<p>Compute summary statisitics of the simulation. Optionally compute block into bins first.</p> Source code in <code>thermotar/thermo.py</code> <pre><code>def stats(self, n_blocks: Optional[int] = None) -&gt; pd.DataFrame:\n    \"\"\"Compute summary statisitics of the simulation. Optionally compute block into bins first.\"\"\"\n    if n_blocks is not None:\n        df = self.block_aves(n_blocks=n_blocks)\n    else:\n        df = self.data\n\n    return df.describe()\n</code></pre>"}]}